
## Analyze the results

Except the logs, all the files generated by the application are in easy to process `csv` or `json` format, and they are all generated in a subfolder of the `output_dir` (by default, `./results`) unique for each benchmark run.

For example:
```text
results/randomforest.test.test.local.20201204T192714
|-- predictions
|   |-- cholesterol
|   |   |-- 0
|   |   |   |-- metadata.json
|   |   |   `-- predictions.csv
|   |   `-- 1
|   |       |-- metadata.json
|   |       `-- predictions.csv
|   |-- iris
|   |   |-- 0
|   |   |   |-- metadata.json
|   |   |   `-- predictions.csv
|   |   `-- 1
|   |       |-- metadata.json
|   |       `-- predictions.csv
|   `-- kc2
|       |-- 0
|       |   |-- metadata.json
|       |   `-- predictions.csv
|       `-- 1
|           |-- metadata.json
|           `-- predictions.csv
`-- scores
    |-- RandomForest.benchmark_test.csv
    `-- results.csv
```

### Results file format

Here is a sample `results.csv` file from a test run against the `RandomForest` framework:

```csv
id,task,framework,constraint,fold,result,metric,mode,version,params,tag,utc,duration,models,seed,info,acc,auc,balacc,logloss,mae,r2,rmse
openml.org/t/3913,kc2,RandomForest,test,0,0.865801,auc,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:27:46,3.2,2000,2633845682,,0.792453,0.865801,0.634199,0.350891,,,
openml.org/t/3913,kc2,RandomForest,test,1,0.86039,auc,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:27:52,3.0,2000,2633845683,,0.90566,0.86039,0.772727,0.406952,,,
openml.org/t/59,iris,RandomForest,test,0,0.126485,logloss,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:27:56,2.9,2000,2633845682,,0.933333,,0.933333,0.126485,,,
openml.org/t/59,iris,RandomForest,test,1,0.0271781,logloss,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:28:01,3.0,2000,2633845683,,1.0,,1.0,0.0271781,,,
openml.org/t/2295,cholesterol,RandomForest,test,0,44.3352,rmse,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:28:05,3.0,2000,2633845682,,,,,,35.6783,-0.014619,44.3352
openml.org/t/2295,cholesterol,RandomForest,test,1,55.3163,rmse,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:28:10,3.1,2000,2633845683,,,,,,43.1808,-0.0610752,55.3163
```
which gives in more readable format:
```text
                  id         task     framework constraint fold     result   metric   mode version                  params                  utc  duration models        seed       acc       auc    balacc   logloss      mae        r2     rmse
0  openml.org/t/3913          kc2  RandomForest       test    0   0.865801      auc  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:27:46       3.2   2000  2633845682  0.792453  0.865801  0.634199  0.350891      NaN       NaN      NaN
1  openml.org/t/3913          kc2  RandomForest       test    1   0.860390      auc  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:27:52       3.0   2000  2633845683  0.905660  0.860390  0.772727  0.406952      NaN       NaN      NaN
2    openml.org/t/59         iris  RandomForest       test    0   0.126485  logloss  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:27:56       2.9   2000  2633845682  0.933333       NaN  0.933333  0.126485      NaN       NaN      NaN
3    openml.org/t/59         iris  RandomForest       test    1   0.027178  logloss  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:28:01       3.0   2000  2633845683  1.000000       NaN  1.000000  0.027178      NaN       NaN      NaN
4  openml.org/t/2295  cholesterol  RandomForest       test    0  44.335200     rmse  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:28:05       3.0   2000  2633845682       NaN       NaN       NaN       NaN  35.6783 -0.014619  44.3352
5  openml.org/t/2295  cholesterol  RandomForest       test    1  55.316300     rmse  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:28:10       3.1   2000  2633845683       NaN       NaN       NaN       NaN  43.1808 -0.061075  55.3163

```

Here is a short description of each column:
- `id`: a identifier for the dataset used in this result. For convenience, we use the link to the OpenML task by default.
- `task`: the task name as defined in the benchmark definition.
- `framework`: the framework name as defined in the framework definition.
- `fold`: the dataset fold being used for this job. Usually, we're using 10 folds, so the fold varies from 0 to 9.
- `result`: the result score, this is the score for the metric that the framework was trying to optimize. For example, for binary classification, the default metrics defined in `resources/config.yaml` are `binary: ['auc', 'acc']`; this means that the frameworks should try to optimize `auc` and the final `auc` score will become the `result` value, the other metrics (here `acc`) are then computed for information.
- `mode`: one of `local`, `docker`, `aws`, `aws+docker`: tells where/how the job was executed.
- `version`: the version of the framework being benchmarked.
- `params`: if any, a JSON representation of the params defined in the framework definition. This allows to see clearly if some tuning was done for example.
- `tag`: the branch tag of the `automlbenchmark` app that was running the job.
- `utc`: the UTC timestamp at the job completion.
- `duration`: the training duration: the framework integration is supposed to provide this information to ensure that it takes only into account the time taken by the framework itself. When benchmarking large data, the application can use a significant amount of time to prepare the data: this additional time doesn't appear in this `duration` column.
- `models`: for some frameworks, it is possible to know how many models in total were trained by the AutoML framework. 
- `seed`: the seed or random state passed to the framework. With some frameworks, it is enough to obtain reproducible results. Note that the seed can be specified at the command line using `-Xseed=` arg (for example `python randomforest -Xseed=1452956522`): when there are multiple folds, the seed is then incremented by the fold number.
- `info`: additional info in text format, this usually contains error messages if the job failed.
- `acc`, `auc`, `logloss` metrics: all the metrics that were computed based on the generated predictions. For each job/row, one of them matches the `result` column, the others are purely informative. Those additional metric columns are simply added in alphabetical order.

### Predictions

For each training, the framework integration must generate a predictions file that will be used by the application to compute the scores.

This predictions file:
- must be saved under the `predictions` subfolder as shown [above](#analyze-the-results).
- follow the naming convention: `{framework}_{task}_{fold}.csv`.
- must be in `CSV` format with first row as header.
- for classification problems, the header should look like: `*class_labels | predictions | truth`, with
  - `*class_labels` meaning an alphabetically ordered list of class/target labels, each column containing the probabilities for the corresponding class. If the framework doesn't provide those probabilities, then the framework integration should provide pseudo-probabilities, using `1` for the predicted value and `0` for other classes.
  - `predictions` column containing the predictions of the test predictor data (`test.X`) by the model trained by the framework,
  - `truth` being the test target data (`test.y`).
- for regression problems, the header should look like `predictions | truth`, with
  - `predictions` column containing the predictions of the test predictor data (`test.X`) by the model trained by the framework, 
  - `truth` being the test target data (`test.y`). 
  
_Examples_:

Predictions sample on `binary` classification (`kc2`):

in CSV format:
```csv
no,yes,predictions,truth
0.965857617846013,0.034142382153998944,no,no
0.965857617846013,0.034142382153998944,no,no
0.5845,0.4155,no,no
0.6795,0.3205,no,no
0.965857617846013,0.034142382153998944,no,no
```
as table:

| no                | yes                  | predictions | truth | 
|-------------------|----------------------|-------------|-------| 
| 0.965857617846013 | 0.034142382153998944 | no          | no    | 
| 0.965857617846013 | 0.034142382153998944 | no          | no    | 
| 0.5845            | 0.4155               | no          | no    | 
| 0.6795            | 0.3205               | no          | no    | 
| 0.965857617846013 | 0.034142382153998944 | no          | no    | 


Predictions sample on `multiclass` classification (`iris`):

in CSV format:
```csv
Iris-setosa,Iris-versicolor,Iris-virginica,predictions,truth
1.0,0.0,0.0,Iris-setosa,Iris-setosa
0.9715,0.028,0.0005,Iris-setosa,Iris-setosa
1.0,0.0,0.0,Iris-setosa,Iris-setosa
1.0,0.0,0.0,Iris-setosa,Iris-setosa
1.0,0.0,0.0,Iris-setosa,Iris-setosa
0.0,1.0,0.0,Iris-versicolor,Iris-versicolor
0.0,0.976,0.024,Iris-versicolor,Iris-versicolor
0.0,0.994,0.006,Iris-versicolor,Iris-versicolor
0.0,0.9925,0.0075,Iris-versicolor,Iris-versicolor
0.0,0.995,0.005,Iris-versicolor,Iris-versicolor
0.0,0.829,0.171,Iris-versicolor,Iris-virginica
0.0,0.008,0.992,Iris-virginica,Iris-virginica
0.0,0.0005,0.9995,Iris-virginica,Iris-virginica
0.0,0.0015,0.9985,Iris-virginica,Iris-virginica
0.0,0.0395,0.9605,Iris-virginica,Iris-virginica
```
as table:

| Iris-setosa | Iris-versicolor | Iris-virginica | predictions     | truth           | 
|-------------|-----------------|----------------|-----------------|-----------------| 
| 1.0         | 0.0             | 0.0            | Iris-setosa     | Iris-setosa     | 
| 0.9715      | 0.028           | 0.0005         | Iris-setosa     | Iris-setosa     | 
| 1.0         | 0.0             | 0.0            | Iris-setosa     | Iris-setosa     | 
| 1.0         | 0.0             | 0.0            | Iris-setosa     | Iris-setosa     | 
| 1.0         | 0.0             | 0.0            | Iris-setosa     | Iris-setosa     | 
| 0.0         | 1.0             | 0.0            | Iris-versicolor | Iris-versicolor | 
| 0.0         | 0.976           | 0.024          | Iris-versicolor | Iris-versicolor | 
| 0.0         | 0.994           | 0.006          | Iris-versicolor | Iris-versicolor | 
| 0.0         | 0.9925          | 0.0075         | Iris-versicolor | Iris-versicolor | 
| 0.0         | 0.995           | 0.005          | Iris-versicolor | Iris-versicolor | 
| 0.0         | 0.829           | 0.171          | Iris-versicolor | Iris-virginica  | 
| 0.0         | 0.008           | 0.992          | Iris-virginica  | Iris-virginica  | 
| 0.0         | 0.0005          | 0.9995         | Iris-virginica  | Iris-virginica  | 
| 0.0         | 0.0015          | 0.9985         | Iris-virginica  | Iris-virginica  | 
| 0.0         | 0.0395          | 0.9605         | Iris-virginica  | Iris-virginica  | 

Predictions sample on `regression` (`cholesterol`):

in CSV format:
```csv
predictions,truth
241.204,207.0
248.9575,249.0
302.278,268.0
225.9215,234.0
226.6995,201.0
```
as table:

| predictions | truth | 
|-------------|-------| 
| 241.204     | 207.0 | 
| 248.9575    | 249.0 | 
| 302.278     | 268.0 | 
| 225.9215    | 234.0 | 
| 226.6995    | 201.0 | 


### Extract more information

For some frameworks, it is also possible to extract more detailed information, in the form of `artifacts` that are saved after the training.

Examples of those artifacts are:
- logs generated by the framework.
- models or descriptions of the models trained by the framework.
- predictions for each of the model trained by the AutoML framework.
- ...

By default, those artifacts are not saved, and all frameworks don't provide the same artifacts, that's why the list of the artifacts that should be extracted can only be specified in the framework definition using the conventional `_save_artifacts` param:

_Examples:_

```yaml
autosklearn_debug:
  extends: autosklearn
  params:
    _save_artifacts: ['models']  # will save models descriptions under the `models` subfolder

H2OAutoML_debug:
  extends: H2OAutoML
  params:
    _save_artifacts: ['leaderboard', 'logs', 'models']  # will save the leaderboard and models under the `models` subfolder, and the H2O logs under `logs` subfolder.

TPOT_debug:
  extends: TPOT
  params:
    _save_artifacts: ['models']
```