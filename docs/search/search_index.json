{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AutoML Benchmark","text":"<p>These are the AutoML Benchmark documentation pages with information on how to configure and use the AutoML Benchmark tool. For first time users, we advise visiting the getting started page.</p> <p>This documentation is accompanied by our website  which has information on our papers, integrated frameworks,  and evaluation results.</p> <p>Help Wanted!</p> <p>We recently switched to generating doc pages with <code>mkdocs-material</code>. In the process, we did our best to make sure to use the additional functionalities to better present the information and make it easy to find through notes, tabs, and other features.</p> <p>It is possible to find parts of the documentation are not updated (correctly), or are not clear. We welcome all help to improve the documentation. If you have a  suggestion on how to improve the documentation, please open an issue. If you find an error, please open an issue or open a pull request directly. Thanks! </p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>If your question is not answered here, please check our Github issue tracker and discussion board.  If you still can not find an answer, please open a Q&amp;A discussion on Github.</p>"},{"location":"faq/#when-will-you-add-framework-x","title":"(When) will you add framework X?","text":"<p>We are currently not focused on integrating additional AutoML systems. However, we process any pull requests that add frameworks and will assist with the integration. The best way to make sure framework X gets included is to start with the integration  yourself or encourage the package authors to do so. For technical details see  Adding an AutoML Framework.</p> <p>It is also possible to open a Github issue indicating the framework you would like added. Please use a clear title (e.g. \"Add framework: X\") and provide some relevant information  (e.g. a link to the documentation). This helps us keep track of which frameworks people are interested in seeing included.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>The AutoML Benchmark is a tool for benchmarking AutoML frameworks on tabular data. It automates the installation of AutoML frameworks, passing it data, and evaluating their predictions.  Our paper describes the design and showcases  results from an evaluation using the benchmark.  This guide goes over the minimum steps needed to evaluate an AutoML framework on a toy dataset.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>These instructions assume that Python 3.9 (or higher)  and git are installed, and are available under the alias <code>python</code> and <code>git</code>, respectively. We recommend Pyenv for managing multiple Python installations, if applicable.</p> <p>First, clone the repository:</p> <pre><code>git clone https://github.com/openml/automlbenchmark.git --branch stable --depth 1\ncd automlbenchmark\n</code></pre> <p>Create a virtual environments to install the dependencies in:</p>  Linux MacOS Windows <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <pre><code>python -m venv ./venv\nvenv/Scripts/activate\n</code></pre> <p>Then install the dependencies:</p> <pre><code>python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\n</code></pre> Note for Windows users <p>The automated installation of AutoML frameworks is done using shell script, which doesn't work on Windows. We recommend you use Docker to run the examples below. First, install and run <code>docker</code>.  Then, whenever there is a <code>python runbenchmark.py ...</code>  command in the tutorial, add <code>-m docker</code> to it (<code>python runbenchmark.py ... -m docker</code>).</p>"},{"location":"getting_started/#running-the-benchmark","title":"Running the Benchmark","text":"<p>To run a benchmark call the <code>runbenchmark.py</code> script specifying the framework to evaluate. See integrated frameworks for a list of supported frameworks, or the adding a frameworking page on how to add your own.</p>"},{"location":"getting_started/#example-a-test-run-with-random-forest","title":"Example: a test run with Random Forest","text":"<p>Let's try evaluating the <code>RandomForest</code> baseline, which uses scikit-learn's random forest:</p>  Linux MacOS Windows <pre><code>python runbenchmark.py randomforest </code></pre> <pre><code>python runbenchmark.py randomforest </code></pre> <p>As noted above, we need to install the AutoML frameworks (and baselines) in a container. Add <code>-m docker</code> to the command as shown: <pre><code>python runbenchmark.py randomforest -m docker\n</code></pre></p> <p>Important</p> <p>Future example usages will only show invocations without <code>-m docker</code> mode, but Windows users will need to run in some non-local mode.</p> <p>After running the command, there will be a lot of output to the screen that reports on what is currently happening. After a few minutes final results are shown and should  look similar to this:</p> <pre><code>Summing up scores for current run:\n               id        task  fold    framework constraint     result      metric  duration      seed\nopenml.org/t/3913         kc2     0 RandomForest       test   0.865801         auc      11.1 851722466\nopenml.org/t/3913         kc2     1 RandomForest       test   0.857143         auc       9.1 851722467\n  openml.org/t/59        iris     0 RandomForest       test  -0.120755 neg_logloss       8.7 851722466\n  openml.org/t/59        iris     1 RandomForest       test  -0.027781 neg_logloss       8.5 851722467\nopenml.org/t/2295 cholesterol     0 RandomForest       test -44.220800    neg_rmse       8.7 851722466\nopenml.org/t/2295 cholesterol     1 RandomForest       test -55.216500    neg_rmse       8.7 851722467\n</code></pre> <p>The result denotes the performance of the framework on the test data as measured by the metric listed in the metric column. The result column always denotes performance  in a way where higher is better (metrics which normally observe \"lower is better\" are converted, which can be observed from the <code>neg_</code> prefix).</p> <p>While running the command, the AutoML benchmark performed the following steps:</p> <ol> <li>Create a new virtual environment for the Random Forest experiment.      This environment can be found in <code>frameworks/randomforest/venv</code> and will be re-used      when you perform other experiments with <code>RandomForest</code>.</li> <li>It downloaded datasets from OpenML complete with a      \"task definition\" which specifies cross-validation folds.</li> <li>It evaluated <code>RandomForest</code> on each (task, fold)-combination in a separate subprocess, where:<ol> <li>The framework (<code>RandomForest</code>) is initialized.</li> <li>The training data is passed to the framework for training.</li> <li>The test data is passed to the framework to make predictions on.</li> <li>It passes the predictions back to the main process</li> </ol> </li> <li>The predictions are evaluated and reported on. They are printed to the console and      are stored in the <code>results</code> directory. There you will find:<ol> <li><code>results/results.csv</code>: a file with all results from all benchmarks conducted on your machine.</li> <li><code>results/randomforest.test.test.local.TIMESTAMP</code>: a directory with more information about the run,     such as logs, predictions, and possibly other artifacts.</li> </ol> </li> </ol> <p>Docker Mode</p> <p>When using docker mode (with <code>-m docker</code>) a docker image will be made that contains the virtual environment. Otherwise, it functions much the same way.</p>"},{"location":"getting_started/#important-parameters","title":"Important Parameters","text":"<p>As you can see from the results above, the  default behavior is to execute a short test benchmark. However, we can specify a different benchmark, provide different constraints, and even run the experiment in a container or on AWS. There are many parameters for the <code>runbenchmark.py</code> script, but the most important ones are:</p> <code>Framework (required)</code> <p>The AutoML framework or baseline to evaluate and is not case-sensitive. See   integrated frameworks for a list of supported frameworks.    In the above example, this benchmarked framework <code>randomforest</code>.</p> <code>Benchmark (optional, default='test')</code> <p>The benchmark suite is the dataset or set of datasets to evaluate the framework on.   These can be defined as on OpenML as a study or task    (formatted as <code>openml/s/X</code> or <code>openml/t/Y</code> respectively) or in a local file.   The default is a short evaluation on two folds of <code>iris</code>, <code>kc2</code>, and <code>cholesterol</code>.</p> <code>Constraints (optional, default='test')</code> <p>The constraints applied to the benchmark as defined by default in constraints.yaml.   These include time constraints, memory constrains, the number of available cpu cores, and more.   Default constraint is <code>test</code> (2 folds for 10 min each). </p> <p>Constraints are not enforced!</p> <p>These constraints are forwarded to the AutoML framework if possible but, except for runtime constraints, are generally not enforced. It is advised when benchmarking to use an environment that mimics the given constraints.</p> Constraints can be overriden by <code>benchmark</code> <p>A benchmark definition can override constraints on a task level. This is useful if you want to define a benchmark which has different constraints for different tasks. The default \"test\" benchmark does this to limit runtime to 60 seconds instead of 600 seconds, which is useful to get quick results for its small datasets. For more information, see defining a benchmark.</p> <code>Mode (optional, default='local')</code> <p>The benchmark can be run in four modes:</p> <ul> <li><code>local</code>: install a local virtual environment and run the benchmark on your machine.</li> <li><code>docker</code>: create a docker image with the virtual environment and run the benchmark in a container on your machine.               If a local or remote image already exists, that will be used instead. Requires Docker.</li> <li><code>singularity</code>: create a singularity image with the virtual environment and run the benchmark in a container on your machine. Requires Singularity.</li> <li><code>aws</code>: run the benchmark on AWS EC2 instances.           It is possible to run directly on the instance or have the EC2 instance run in <code>docker</code> mode.           Requires valid AWS credentials to be configured, for more information see Running on AWS.</li> </ul> <p>For a full list of parameters available, run:</p> <pre><code>python runbenchmark.py --help\n</code></pre>"},{"location":"getting_started/#example-automl-on-a-specific-task-and-fold","title":"Example: AutoML on a specific task and fold","text":"<p>The defaults are very useful for performing a quick test, as the datasets are small and cover different task types (binary classification, multiclass classification, and  regression). We also have a \"validation\" benchmark suite for more elaborate testing that also includes missing data, categorical data,  wide data, and more. The benchmark defines 9 tasks, and evaluating two folds with a 10-minute time constraint would take roughly 3 hours (=9 tasks * 2 folds * 10 minutes, plus overhead). Let's instead use the <code>--task</code> and <code>--fold</code> parameters to run only a specific task and fold in the <code>benchmark</code> when evaluating the  flaml AutoML framework:</p> <pre><code>python runbenchmark.py flaml validation test -t eucalyptus -f 0\n</code></pre> <p>This should take about 10 minutes plus the time it takes to install <code>flaml</code>. Results should look roughly like this:</p> <pre><code>Processing results for flaml.validation.test.local.20230711T122823\nSumming up scores for current run:\n               id       task  fold framework constraint    result      metric  duration       seed\nopenml.org/t/2079 eucalyptus     0     flaml       test -0.702976 neg_logloss     611.0 1385946458\n</code></pre> <p>Similarly to the test run, you will find additional files in the <code>results</code> directory.</p>"},{"location":"getting_started/#example-benchmarks-on-openml","title":"Example: Benchmarks on OpenML","text":"<p>In the previous examples, we used benchmarks which were defined in a local file (test.yaml and validation.yaml, respectively). However, we can also use tasks and benchmarking suites defined on OpenML directly from the command line. When referencing an OpenML task or suite, we can use <code>openml/t/ID</code> or <code>openml/s/ID</code> respectively as  argument for the benchmark parameter. Running on the iris task:</p> <pre><code>python runbenchmark.py randomforest openml/t/59\n</code></pre> <p>or on the entire AutoML benchmark classification suite (this will take hours!):</p> <pre><code>python runbenchmark.py randomforest openml/s/271\n</code></pre> <p>Large-scale Benchmarking</p> <p>For large scale benchmarking it is advised to parallelize your experiments, as otherwise it may take months to run the experiments. The benchmark currently only supports native parallelization in <code>aws</code> mode (by using the <code>--parallel</code> parameter), but using the <code>--task</code> and <code>--fold</code> parameters  it is easy to generate scripts that invoke individual jobs on e.g., a SLURM cluster. When you run in any parallelized fashion, it is advised to run each process on separate hardware to ensure experiments can not interfere with each other.</p>"},{"location":"legacy/","title":"Legacy","text":""},{"location":"legacy/#add-a-benchmark","title":"Add a benchmark","text":"<p>In this section, <code>benchmark</code> means a suite of datasets that can be used to feed any of the available frameworks, in combination with a set of constraints (time limit, cpus, memory) enforced by the application.</p> <p>A benchmark definition will then consist in a datasets definition and a constraints definition.</p> <p>Each dataset must contain a training set and a test set. There can be multiple training/test splits, in which case each split is named a <code>fold</code>, so that the same dataset can be benchmarked multiple times using a different fold.</p> <p>or using pyenv: <pre><code>pyenv install {python_version: 3.9.16}\npyenv virtualenv ve-automl\npyenv local ve-automl\n</code></pre></p> <ul> <li>NOTE: in case of issues when installing Python requirements, you may want to try the following:<ul> <li>on some platforms, we need to ensure that requirements are installed sequentially: <code>xargs -L 1 python -m pip install &lt; requirements.txt</code>.</li> <li>enforce the <code>python -m pip</code> version above in your virtualenv: <code>python -m pip install --upgrade pip==19.3.1</code>.</li> </ul> </li> </ul>"},{"location":"legacy/#troubleshooting-guide","title":"Troubleshooting guide","text":""},{"location":"legacy/#where-are-the-results","title":"Where are the results?","text":"<p>By default, the results for a benchmark execution are made available in a subfolder under <code>output_dir</code> (if not specified by <code>-o my_results</code>, then this is under <code>{cwd}/results</code>).</p> <p>This subfolder is named <code>{framework}_{benchmark}_{constraint}_{mode}_{timestamp}</code>.</p> <p>So that for example: <pre><code>python runbenchmark.py randomforest\n</code></pre> will create a subfolder <code>randomforest_test_test_local_20200108T184305</code>,</p> <p>and: <pre><code>python runbenchmark.py randomforest validation 1h4c -m aws\n</code></pre> will create a subfolder <code>randomforest_validation_1h4c_aws_20200108T184305</code>.</p> <p>Then each subfolder contains:  - a <code>score</code> folder with a <code>results.csv</code> file concatenating the results from all the tasks in the benchmark, as well as potentially other individual results for each task.  - a <code>predictions</code> folder with the predictions for each task in the benchmark.  - a <code>logs</code> folder: only if benchmark was executed with <code>-o output_dir</code> argument.  - possibly more folders if the framework saves additional artifacts.</p> <p>Also the <code>output_dir</code> contains a <code>results.csv</code> concatenating ALL results from all subfolders.</p>"},{"location":"legacy/#where-are-the-logs","title":"Where are the logs?","text":"<p>By default the application logs are available under <code>{cwd}/logs</code> if the benchmark is executed without specifying the <code>output_dir</code>, otherwise, they'll be available under the <code>logs</code> subfolder in the benchmark results (see Where are the results?).</p> <p>The application can collect various logs: - local benchmark application logs: those are always collected. For each run, the application generated 2 log files locally:   - <code>runbenchmark_{timestamp}.log</code>: contains logs for the application only (from DEBUG level).   - <code>runbenchmark_{timestamp}_full.log</code>: contains logs for the application + other Python libraries (from INFO level); e.g. <code>boto3</code> logs when running in <code>aws</code> mode. - remote application logs: for <code>aws</code> mode only, logs generated on the remote instances are automatically downloaded to the results folder, together with other result artifacts.  - framework logs (optional): if the framework integration supports it, it is possible to ask for the framework logs by creating a custom framework definition as follow:   <pre><code>H2OAutoML:\nextends: H2OAutoML\nparams:\n_save_artifacts: ['logs']\n</code></pre></p>"},{"location":"legacy/#profiling-the-application","title":"Profiling the application","text":"<p>Currently, the application provides a global flag <code>--profiling</code> to activate profiling for some specific methods that can be slow or memory intensive: <pre><code>python runbenchmark.py randomforest --profiling\n</code></pre> All methods/functions are not profiled, but if you need to profile more, you just need to decorate the function with the <code>@profile()</code> decorator (from <code>amlb.utils</code>).</p>"},{"location":"legacy/#memory-usage","title":"Memory usage","text":"<p>Examples of memory info when using this custom profiling: <pre><code>[PROFILING] `amlb.datasets.openml.OpenmlDatasplit.data` returned object size: 45.756 MB.\n[PROFILING] `amlb.datasets.openml.OpenmlDatasplit.data` memory change; process: +241.09 MB/379.51 MB, resident: +241.09 MB/418.00 MB, virtual: +230.01 MB/4918.16 MB.\n...\n[PROFILING] `amlb.data.Datasplit.release` executed in 0.007s.\n[PROFILING] `amlb.data.Datasplit.release` memory change; process: -45.73 MB/238.80 MB, resident: +0.00 MB/414.60 MB, virtual: +0.00 MB/4914.25 MB.\n</code></pre></p>"},{"location":"legacy/#methods-duration","title":"Methods duration","text":"<p>Examples of method duration info when using this custom profiling: <pre><code>[PROFILING] `amlb.datasets.openml.OpenmlLoader.load` executed in 7.456s.\n...\n[PROFILING] `amlb.data.Datasplit.X_enc` executed in 6.570s.\n</code></pre></p>"},{"location":"legacy/#python-library-version-conflict","title":"Python library version conflict","text":"<p>see Framework integration</p>"},{"location":"legacy/#framework-setup-is-not-executed","title":"Framework setup is not executed","text":"<p>Try the following: - force the setup using the <code>-s only</code> or <code>-s force</code> arg on the command line:   - <code>-s only</code> or <code>--setup=only</code> will force the setup and skip the benchmark run.   - <code>-s force</code> or <code>--setup=force</code> will force the setup and run the benchmark immediately. - delete the <code>.marker_setup_safe_to_delete</code> from the framework module and try to run the benchmark again. This marker file is automatically created after a successful setup to avoid having to execute it each tine (setup phase can be time-consuming), this marker then prevents auto-setup, except if the <code>-s only</code> or <code>-s force</code> args above are used.</p>"},{"location":"legacy/#framework-setup-fails","title":"Framework setup fails","text":"<p>If the setup fails, first note that only the following OS are fully supported: - Ubuntu 18.04</p> <p>The setup is created for Debian-based linux environments, and macOS (most frameworks can be installed on macOS, ideally with <code>brew</code> installed, but there may be a few exceptions), so it may work with other Linux environments not listed above (e.g. Debian, Ubuntu 20.04, ...). The best way to run benchmarks on non-supported OS, is to use the docker mode.</p> <p>If the setup fails on a supported environment, please try the following: - force the setup: see above. - ensure that the same framework is not set up multiple times in parallel on the same machine:   - first use <code>python runbenchmark.py MyFramework -s only</code> on one terminal.   - then you can trigger multiple <code>python runbenchmark.py MyFramework ...</code> (without <code>-s</code> option) in parallel. - delete the <code>lib</code> and <code>venv</code> folders, if present, under the given framework folder (e.g. <code>frameworks/MyFramework</code>), and try the setup again.</p>"},{"location":"extending/","title":"Extending the Benchmark Tool","text":"<p>You can extend the benchmark tool in multiple ways. Benchmarks define collections of tasks on which to evaluate AutoML frameworks. Constraints specify the resource constraints forwarded to the AutoML framework, such as a time or memory limit. Finally, it is possible to add AutoML frameworks or to  use an integrated AutoML framework with non-default configuration.</p>"},{"location":"extending/benchmark/","title":"Benchmark","text":"<p>Benchmarks are collections of machine learning tasks, where each task is a dataset with associated information on train/test splits used to evaluate the model. These tasks can be defined in a <code>yaml</code> file or on OpenML. Both options allow for defining a benchmark of one or more datasets. It is even possible to reference to OpenML tasks from a benchmark file.</p> <p>Supported Datasets</p> <p>Currently, the AutoML benchmark only supports definitions of tabular datasets for classification, regression, and time series forecasting. The time series forecasting support is in an early stage, subject to change, and not supported through OpenML.</p>"},{"location":"extending/benchmark/#defining-a-benchmark-on-openml","title":"Defining a Benchmark on OpenML","text":"<p>Especially when performing a benchmark evaluation to be used in a publication, we recommend the use of OpenML for the definition of the benchmark if possible. This ensures that other users can run your benchmark out of the box, without any required additional files. OpenML also provides a lot of meta-data about the datasets which is also accessible through APIs in various programming  languages. We recommend using the <code>openml-python</code> Python library as it is the most comprehensive of the OpenML libraries.</p> <p>Defining a benchmark on OpenML requires the following steps:</p> <ul> <li>Upload a dataset.     A dataset is the tabular data, alongside meta-data like its name,    authors, and license. OpenML will also automatically extract meta-data about the    datasets, such as feature types, class balance, or dataset size. After uploading the    dataset, it will receive an identifier (<code>ID</code>) and should be visible on the OpenML    website on <code>www.openml.org/d/ID</code>.</li> <li>Define a task.     A task defines how to evaluate a model on a given dataset, for example    \"10-fold cross-validation optimizing AUC\". OpenML will generate splits for the 10-fold    cross-validation procedure which means that anyone using this task definition can     perform the experiment with the exact same splits easily.</li> <li>Define a benchmark suite.     On a technical level, a benchmarking suite is nothing more than a collection of tasks.     You can add a description that details the purpose of the benchmarking suite, or any     information that users should be aware of before using the suite.</li> </ul> <p>When a task or benchmark suite is available on OpenML, it can be directly referred to for the <code>benchmark</code> parameter of <code>runbenchmark.py</code> as <code>openml/s/ID</code> for suites and  <code>openml/t/ID</code> for tasks, where <code>ID</code> is to be replaced with the OpenML identifier of the object. For example, <code>openml/t/59</code> refers to task 59,  which is 10-fold cross-validation on the iris dataset.</p>"},{"location":"extending/benchmark/#defining-a-benchmark-with-a-file","title":"Defining a Benchmark with a File","text":"<p>When defining a benchmark with a <code>yaml</code> file, the <code>yaml</code> will contain information about tasks that are located either on disk or on OpenML. We make a few default benchmarks available in our <code>resources/benchmarks</code> folder:</p> <ul> <li><code>test</code>: a collection of three small datasets covering regression, binary classification,      and multiclass classification. This makes it incredibly useful for small tests and     fast feedback on whether the software runs without error.</li> <li><code>validation</code>: a collection of datasets which have different edge cases, such as a     very wide dataset, datasets with missing or non-numerical values, and more. This     typically produces most errors you might also encounter when running larger      benchmarks.</li> <li><code>timeseries</code>: a benchmark for testing time series forecasting integration (experimental).</li> </ul> <p>Below is an excerpt from the <code>test.yaml</code> file:</p> <pre><code>- name: kc2\nopenml_task_id: 3913\ndescription: \"binary test dataset\"\n</code></pre> <p>When writing your own benchmark definition, it needs to be discoverable by the benchmark. A good place to do this would be adding a <code>benchmarks</code> directory to your benchmark configuration directory (<code>~/.config/automlbenchmark</code> by default) and updating your custom configuration by adding:</p> <pre><code>benchmarks:                     definition_dir:               - '{root}/resources/benchmarks'\n- '{user}/resources/benchmarks'\n</code></pre> <p>Each task must have a name that is unique in the definition file (case-insensitive), this name will also be used as identifier (e.g., in the results files). Additionally, the file must have a description of where to find the dataset files and splits. When you have a task already on OpenML, you can directly reference it with <code>openml_task_id</code> to define the dataset and splits. Alternatively, you can use local files.</p> <p>It is also possible to benchmark your own datasets that you can not or do not want to upload to OpenML. The data files should be in <code>arff</code> or <code>csv</code> format and contain at least  one file for training data and one file for test data. When working with multiple files, it is useful to use an archive (<code>.zip</code>, <code>.tar</code>, <code>.tgz</code>, <code>.tbz</code>) or directory structure.  Use the following naming convention to allow the AutoML benchmark to infer what each file represents:</p> <pre><code>- if there's only one file for training and one for test, they should be named `{name}_train.csv` and `{name}_test.csv` (in case of CSV files).\n- if there are multiple `folds`, they should follow a similar convention: `{name}_train_0.csv`, `{name}_test_0.csv``, {name}_train_1.csv`, `{name}_test_1.csv`, ...\n</code></pre> <p>Examples:</p> Single Fold CSVMultiple Folds CSVDirectoryArchiveRemote Files <pre><code>- name: example_csv\ndataset:\ntrain: /path/to/data/ExampleTraining.csv\ntest:  /path/to/data/ExampleTest.csv\ntarget: TargetColumn\nfolds: 1\n</code></pre> <pre><code>- name: example_multi_folds\ndataset:\ntrain: - /path/to/data/ExampleTraining_0.csv\n- /path/to/data/ExampleTraining_1.csv\ntest:  - /path/to/data/ExampleTest_0.csv\n- /path/to/data/ExampleTest_1.csv\ntarget: TargetColumn\nfolds: 2\n</code></pre> <p>It is important that the files in the directory follow the naming convention described above.</p> <pre><code>- name: example_dir\ndataset: path: /path/to/data\ntarget: TargetColumn\nfolds: 1\n</code></pre> <p>It is important that the files in the archive follow the naming convention described above.</p> <pre><code>- name: example_archive\ndataset:\npath: /path/to/archive.zip\ntarget: TargetColumn\nfolds: 3\n</code></pre> <p>The remote file may also be an archive. If that is the case, it is important that  the files in the archive follow the naming convention described above.</p> <pre><code>- name:  example_csv_http\ndataset:\ntrain: https://my.domain.org/data/ExampleTraining.csv\ntest:  https://my.domain.org/data/ExampleTest.csv\ntarget: TargetColumn\nfolds: 1\n</code></pre> <p>Remote files are downloaded to the <code>input_dir</code> folder and archives are decompressed  there as well. You can change the value of this folder in your  custom config.yaml file  or specify it at the command line with the <code>-i</code> or <code>--indir</code> argument  (by default, it points to the <code>~/.openml/cache</code> folder).</p> <p>The <code>target</code> attribute is optional but recommended. If not set, it will resolve to the  column <code>target</code> or <code>class</code> if present, and the last column otherwise.</p> <p>You can even make use of the special directives like <code>{user}</code>.</p> <pre><code>- name: example_relative_to_user_dir\ndataset:\ntrain: \"{user}/data/train.csv\"\ntest: \"{user}/data/test.csv\"\n</code></pre> <p>After creating a benchmark definition, e.g. <code>~/.config/automlbenchmark/benchmarks/my_benchmark.yaml</code>, it can then be referenced when running <code>runbenchmark.py</code>: <code>python runbenchmark.py FRAMEWORK my_benchmark</code>.</p>"},{"location":"extending/constraint/","title":"Constraints","text":"<p>Constraint definitions allow a set of common constraints to be applied to all tasks in  a benchmark. Default constraint definitions are available in  <code>resources/constraint.yaml</code>. When no constraint is specified at the command line, the <code>test</code> constraint definition is used by default.</p> <p>A constraint definition can consist of the following constraints:</p> <ul> <li><code>folds</code> (default=10): The number of folds to evaluate for the task. Has to be less or equal to the number of folds defined by the task.</li> <li><code>max_runtime_seconds</code> (default=3600): maximum time in seconds for each individual fold of a benchmark task.    This parameter is usually passed directly to the framework. If it doesn't respect the    constraint, the application will abort the task after <code>2 * max_runtime_seconds</code>.    In any case, the actual time used is always recorded and available in the results.</li> <li><code>cores</code> (default=-1): amount of cores used for each automl task. If non-positive, it will try to use all cores.</li> <li><code>max_mem_size_mb</code> (default=-1): amount of memory assigned to each automl task.     If non-positive, then the amount of memory is computed from os available memory.</li> <li><code>min_vol_size_mb</code> (default=-1): minimum amount of free space required on the volume. If non-positive, skips verification. If the requirement is not fulfilled, a warning message will be printed, but the task will still be attempted.</li> <li><code>ec2_volume_type</code>: The volume type to use for the task when using EC2 instances, otherwise defaults to the value of <code>aws.ec2.volume_type</code> in your configuration file.</li> </ul> <p>Constraints are not enforced!</p> <p>These constraints are forwarded to the AutoML framework if possible but are  generally not enforced. Not all AutoML frameworks allow for e.g., memory limits to be set, and not all implementations that do treat it as a hard constraint. For that reason, only <code>max_runtime_seconds</code> is enforced as described above. It is advised when benchmarking to use an environment that mimics the given constraints.</p> Constraints can be overriden by <code>benchmark</code> <p>A benchmark definition can override constraints on a task level. This is useful if you want to define a benchmark which has different constraints for different tasks. The default \"test\" benchmark does this to limit runtime to 60 seconds instead of 600 seconds, which is useful to get quick results for its small datasets. For more information, see defining a benchmark.</p> <p>When writing your own constraint definition, it needs to be discoverable by the benchmark. A good place to do this would be adding a <code>constraints.yaml</code> file to your benchmark configuration directory (<code>~/.config/automlbenchmark</code> by default) and updating your custom configuration by adding:</p> <pre><code>benchmarks:                     constraints_file: - '{root}/resources/constraints.yaml'\n- '{user}/constraints.yaml'\n</code></pre> <p>You can then define multiple constraints in your constraint file, for example: {user}/constraints.yaml<pre><code>---\ntest:\nfolds: 1\nmax_runtime_seconds: 120\n8h16c:\nfolds: 10\nmax_runtime_seconds: 28800\ncores: 16\nmin_vol_size_mb: 65536\nec2_volume_type: gp3\n</code></pre></p> <p>The new constraints can now be passed on the command line when executing the benchmark: <pre><code>python runbenchmark.py randomforest validation 8h16c\n</code></pre> Note: The above example is allowed to run for 8 hours, but will stop earlier as  <code>RandomForest</code> stops early after training 2000 trees.</p>"},{"location":"extending/framework/","title":"Adding an AutoML Framework","text":"<p>Rewrite in progress</p> <p>Most information on this page is accurate, and it should be complete enough to use. However, it hasn't been updated to make use of <code>mkdocs-materials</code> features, and might have some outdated examples. Contributions welcome.</p>"},{"location":"extending/framework/#add-an-automl-framework","title":"Add an AutoML framework","text":"<p>Adding an AutoML framework consist in several steps:</p> <ol> <li>create a Python module that will contain everything related to the integration of this framework.</li> <li>define the framework in a Framework definition file.</li> <li>write some integration code</li> <li>to download/setup the framework dynamically: by convention, this is done by a <code>setup.sh</code> script defined in the module.</li> <li>to run the framework using the data and constraints/parameters provided by the benchmark application: by convention, this is done by an <code>exec.py</code> script in the module, but it may require more files depending on the framework, for example if it runs on Python or R, Java...</li> </ol>"},{"location":"extending/framework/#framework-definition","title":"Framework definition","text":"<p>The framework definition consists in an entry in a <code>yaml</code> file with the framework name and some properties</p> <ol> <li>to describe the framework and define which version will be used: <code>project</code>, <code>version</code>.</li> <li>to indicate the Python module with the integration code: <code>module</code> or <code>extends</code>.</li> <li>to pass optional parameters to the framework and/or the integration code: <code>params</code>.</li> </ol> <p>Default framework definitions are defined in file <code>resources/frameworks.yaml</code> in lexicographic order, where <code>version</code> should be set to <code>stable</code>, which will point dynamically to the most recent official release available.</p> <p>Frameworks that offer the possibility to test cutting edge version (e.g. nightly builds, <code>dev</code>/<code>master</code> repo, ...) can add an entry to <code>resources/frameworks_latest.yaml</code>, where <code>version</code> should be set to <code>latest</code>.</p> <p>Maintainers of this repository try to regularly \u2014 ideally, every quarter \u2014 create a framework definition using frozen framework versions in order to favour the reproducibility of the published benchmarks.</p> <p>Following the custom configuration, it is possible to override and/or add a framework definitions by creating a <code>frameworks.yaml</code> file in your <code>user_dir</code>.</p> <p>See for example the <code>examples/custom/frameworks.yaml</code>:</p> <pre><code>---\nGradientBoosting:\nmodule: extensions.GradientBoosting\nproject: https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting\nparams:\nn_estimators: 500\nStacking:\nmodule: extensions.Stacking\nproject: https://scikit-learn.org/stable/modules/ensemble.html#stacking\nparams:\n_rf_params: {n_estimators: 200}\n_gbm_params: {n_estimators: 200}\n_linear_params: {penalty: elasticnet, loss: log}\n#    _svc_params: {tol: 1e-3, max_iter: 1e5}\n#    _final_params: {penalty: elasticnet, loss: log} # sgd linear\n_final_params: {max_iter: 1000}  # logistic/linear\nautosklearn_latest:\nextends: autosklearn\nversion: latest\ndescription: \"this will use master branch from the autosklearn repository instead of the fixed version\"\nautosklearn_mybranch:\nextends: autosklearn\nversion: mybranch\ndescription: \"this will use mybranch branch from the autosklearn repository instead of the fixed version\"\nautosklearn_oldgen:\nextends: autosklearn\nversion: \"0.7.1\"\ndescription: \"this will use the latest autosklearn version from the old generation\"\nH2OAutoML_nightly:\nmodule: frameworks.H2OAutoML\nsetup_cmd: 'LATEST_H2O=`curl http://h2o-release.s3.amazonaws.com/h2o/master/latest` &amp;&amp; pip install --no-cache-dir -U \"http://h2o-release.s3.amazonaws.com/h2o/master/${{LATEST_H2O}}/Python/h2o-3.29.0.${{LATEST_H2O}}-py2.py3-none-any.whl\"'\nversion: 'nightly'\nH2OAutoML_custom:\nextends: H2OAutoML\nparams:\nnfolds: 3\nstopping_tolerance: 0.05\n</code></pre> <p>This example shows</p> <ul> <li>the definitions for 2 new frameworks: <code>GradientBoosting</code> and <code>Stacking</code>. </li> <li>Those definitions (optionally) externalize some parameters (e.g. <code>n_estimators</code>): the <code>params</code> property always appears in json format in the results, so that we can clearly see what has been tuned when analyzing the results later.</li> <li>Note that the module is case sensitive and should point to the module containing the integration code.</li> <li>The application will search for modules from the sys path, which includes the application <code>root_dir</code> and the <code>user_dir</code>: <ul> <li>that's why the default frameworks use <code>module: frameworks.autosklearn</code> for example, </li> <li>and the example above can use <code>module: extensions.GradientBoosting</code> because those examples must be run by setting the <code>user_dir</code> to <code>examples/config</code>, e.g.  <p><code>python runbenchmark.py gradientboosting -u examples/custom</code>.</p> </li> </ul> </li> <li>a custom definition (<code>H2OAutoML_nightly</code>) for the existing <code>frameworks.H2OAutoML</code> module, allowing to reuse the module for a dynamic version of the module:<ul> <li>the <code>setup_cmd</code> is executed after the default setup of the module, so it can be used to make additional setup. To customize the setup, it is possible to use:</li> <li><code>setup_args: my_version</code> (only if the <code>setup.sh</code> in the framework module supports new arguments).</li> <li><code>setup_cmd</code> (as shown in this example). </li> <li><code>setup_script: my_additional_setup.sh</code>.</li> </ul> </li> <li>2 custom definitions (<code>H2OAutoML_blending</code> and <code>H2OAutoML_custom</code>) simply extending the existing <code>H2OAutoML</code> definition (therefore inheriting from all its properties, including the <code>module</code> one), but overriding the <code>params</code> property, thus allowing to provide multiple \"flavours\" of the same framework.  </li> </ul> <p>The frameworks defined in this example can then be used like any other framework as soon as both the framework module and the definition file are made available to the application: in our case, this is done by the creation of the integration modules under <code>examples/custom/extensions</code> and by exposing the definitions in <code>examples/custom/frameworks.yaml</code> thanks to the entry in <code>examples/custom/config.yaml</code>: <pre><code>frameworks:\ndefinition_file:  # this allows to add custom framework definitions (in {user}/frameworks.yaml) on top of the default ones.\n- '{root}/resources/frameworks.yaml'\n- '{user}/frameworks.yaml'\n</code></pre></p> <p>By pointing the <code>user_dir</code> to <code>examples/custom</code>, our <code>config.yaml</code> is also loaded, and we can use the new frameworks: <pre><code>python runbenchmark.py gradientboosting -u examples/custom\npython runbenchmark.py stacking -u examples/custom\npython runbenchmark.py h2oautoml_blending -u examples/custom\n</code></pre></p> <p>Note:</p> <p>By default, when generating a docker image, the image name is created as <code>automlbenchmark/{framework}:{version}-{branch}</code> with the framework name in lowercase, and <code>branch</code> being the branch of the <code>automlbenchmark</code> app (usually <code>stable</code>). However, it is possible to customize this image name as follow: <pre><code>MyFramework:\nversion: 1.0\nmodule: extensions.MyFramework\ndocker:\nauthor: my_docker_repo\nimage: my_image\ntag: my_tag\n</code></pre> which will result in the docker image name <code>my_docker_repo/my_image:my_tag-{branch}</code>, with <code>branch</code> still being the branch of the application.</p>"},{"location":"extending/framework/#framework-integration","title":"Framework integration","text":"<p>If the framework definition allows to use the new framework from the application, the (not so) hard part is to integrate it.</p> <p>There are already several frameworks already integrated under <code>frameworks</code> directory (+ the examples under <code>examples/custom</code>), so the best starting point when adding a new framework is to first look at the existing ones.</p> <p>Among the existing frameworks, we can see different type of integrations:</p> <ul> <li>trivial integration: these are frameworks running on Python and using dependencies (<code>numpy</code>, <code>sklearn</code>) already required by the application itself. These are not really AutoML toolkits, but rather integrations using <code>sklearn</code> to provide a reference when analyzing the results: cf. <code>constantpredictor</code>, <code>DecisionTree</code>.</li> <li>Python API integration: these are frameworks that can be run directly from Python: cf. <code>autosklearn</code>, <code>H2OAutoML</code>, <code>TPOT</code>, <code>RandomForest</code>, <code>TunedRandomForest</code>.</li> <li>contrary to the trivial integration, those require a <code>setup</code> phase.</li> <li>Most of them currently run using the same dependencies as the application, which is not recommended due to potential version conflicts (especially with <code>sklearn</code>). This was not a major constraint with the first frameworks implemented, but now, those integrations can and will be slightly changed to run in their dedicated virtual environment, using their own dependencies: cf. <code>RandomForest</code> and <code>examples/custom/extensions/Stacking</code> for examples.</li> <li>non-Python frameworks: those frameworks typically run in <code>R</code> or <code>Java</code> and don't provide any Python API. The integration is then still done by spawning the <code>Java</code> or <code>R</code> process from the <code>exec.py</code>: cf. <code>AutoWEKA</code> or <code>ranger</code>, respectively.</li> </ul>"},{"location":"extending/framework/#recommended-structure","title":"Recommended structure","text":"<p>By convention, the integration is done using the following structure:</p> <pre><code>frameworks/autosklearn/\n|-- __init__.py\n|-- exec.py\n|-- requirements.txt\n`-- setup.sh\n</code></pre> <p>Please note however, that this structure is not a requirement, the only requirement is the contract exposed by the integration module itself, i.e. by the <code>__init__.py</code> file.</p> <p>A simple <code>__init__.py</code> would look like this:</p> <pre><code>from amlb.utils import call_script_in_same_dir\ndef setup(*args, **kwargs):\ncall_script_in_same_dir(__file__, \"setup.sh\", *args, **kwargs)\ndef run(*args, **kwargs):\nfrom .exec import run\nreturn run(*args, **kwargs)\n</code></pre> <p>where we see that the module should expose (only <code>run</code> is actually required) the following functions:</p> <ul> <li><code>setup</code> (optional): called by the application to setup the given framework, usually by simply running a <code>setup.sh</code> script that will be responsible for potentially creating a local virtual env, downloading and installing the dependencies.     The <code>setup</code> function can also receive the optional <code>setup_args</code> param from the framework definition as an argument. </li> <li><code>run</code>: called by the benchmark application to execute a task against the framework, using the selected dataset and constraints. We will describe the parameters in detail below, for now, just note that by convention, we just load the <code>exec.py</code> file from the module and delegate the execution to its <code>run</code> function.</li> <li><code>docker_commands</code> (optional): called by the application to collect docker instructions that are specific to the framework. If the framework requires a <code>setup</code> phase, then the string returned by this function should at least ensure that the setup is also executed during the docker image creation, that's one reason why it is preferable to do all the setup in a <code>setup.sh</code> script, to allow the docker support above.</li> </ul>"},{"location":"extending/framework/#frameworks-with-python-api","title":"Frameworks with Python API","text":""},{"location":"extending/framework/#frameworks-requiring-a-dedicated-virtual-env","title":"Frameworks requiring a dedicated virtual env","text":"<p>For frameworks with Python API, we may worry about version conflicts between the packages used by the application (e.g. <code>sklearn</code>, <code>numpy</code>, <code>pandas</code>) and the ones required by the framework.</p> <p>In this case, the integration is slightly different as you can see with the <code>RandomForest</code> integration allowing to use any version of <code>sklearn</code>.</p> <p>This is the basic structure after the creation of the dedicated Python virtual environment during setup: <pre><code>frameworks/RandomForest/\n|-- __init__.py\n|-- exec.py\n|-- requirements.txt\n|-- setup.sh\n`-- venv/\n    `-- (this local virtual env is created by the frameworks/shared/setup.sh)\n</code></pre></p> <p>Noticeable differences with a basic integration:</p> <ul> <li>the <code>venv</code> is created in <code>setup.sh</code> by passing the current dir when sourcing the <code>shared/setup.sh</code> script: <code>. $HERE/../shared/setup.sh $HERE</code>.</li> <li>the <code>run</code> function in <code>__init__.py</code> prepares the data (in the application environment) before executing the <code>exec.py</code> in the dedicated <code>venv</code>. The call to <code>run_in_venv</code> is in charge of serializing the input, calling <code>exec.py</code> and deserializing + saving the results from <code>exec</code>.</li> <li><code>exec.py</code>, when calls in the subprocess (function <code>__main__</code>), calls <code>call_run(run)</code> which deserializes the input (dataset + config) and passes it to the <code>run</code> function that just need to return a <code>result</code> object.</li> </ul> <p>Note A:</p> <p>As the serialization/deserialization of <code>numpy</code> arrays can be costly for very large datasets, it is recommended to use dataset serialization only if the framework itself doesn't support loading datasets from files. </p> <p>This means that, in the <code>__init__.py</code> instead of implementing <code>run</code> as: <pre><code>data = dict(\ntrain=dict(\nX=dataset.train.X,\ny=dataset.train.y\n),\ntest=dict(\nX=dataset.test.X,\ny=dataset.test.y\n)\n)\nreturn run_in_venv(__file__, \"exec.py\",\ninput_data=data, dataset=dataset, config=config)\n</code></pre> it could simply expose the dataset paths (the application avoids loading the data if not explicitly needed by the framework): <pre><code>data = dict(\ntarget=dict(name=dataset.target.name),\ntrain=dict(path=dataset.train.path),\ntest=dict(path=dataset.test.path)\n)\nreturn run_in_venv(__file__, \"exec.py\",\ninput_data=data, dataset=dataset, config=config)\n</code></pre></p> <p>Note B:</p> <p>The serialization/deserialization of data between the main process and the framework process can be customized using the <code>options</code> parameter: The allowed options for (de)serialization are defined by the object <code>amlb.utils.serialization.ser_config</code>.</p> <p>For example: <pre><code>data = dict(\ntrain=dict(\nX=dataset.train.X,\ny=dataset.train.y\n),\ntest=dict(\nX=dataset.test.X,\ny=dataset.test.y\n)\n)\noptions = dict(\nserialization=dict(sparse_dataframe_deserialized_format='dense')\n)\nreturn run_in_venv(__file__, \"exec.py\",\ninput_data=data, dataset=dataset, config=config, options=options)\n</code></pre></p>"},{"location":"extending/framework/#other-frameworks","title":"Other Frameworks","text":"<p>Integration of frameworks without any Python API is done in similar way, for example:</p> <p><pre><code>frameworks/AutoWEKA/\n|-- __init__.py\n|-- exec.py\n|-- requirements.txt\n|-- setup.sh\n`-- lib/\n    `-- (this is where the framework dependencies go, usually created by setup.sh)\n</code></pre> or <pre><code>frameworks/ranger/\n|-- __init__.py\n|-- exec.R\n|-- exec.py\n|-- requirements.txt\n`-- setup.sh\n</code></pre></p> <p>Here are the main differences: - the <code>setup</code> phase is identical, but if at runtime, some executable file or library is required that need to be installed locally (as opposed to globally: for example, <code>R</code> or <code>java</code> executable are usually installed globally), we just recommend to put everything under the integration module (for example in <code>lib</code> and/or <code>bin</code> subfolders as for <code>AutoWEKA</code>). This is also true for some Python frameworks (cf. <code>hyperoptsklearn</code> integration for example, where the modules are loaded from <code>frameworks/hyperoptsklearn/lib/hyperopt-sklearn</code>). - the framework is then executed by building a command manually in <code>exec.py</code>, running it in a subprocess, and finally collecting the results generated by the subprocess. For example, in <code>ranger/exec.py</code>:   <pre><code>with Timer() as training:\nrun_cmd((\"Rscript --vanilla -e \\\"\"\n\"source('{script}'); \"\n\"run('{train}', '{test}', '{output}', cores={cores}, meta_results_file='{meta_results}', task_type='{task_type}')\"\n\"\\\"\").format(\nscript=os.path.join(here, 'exec.R'),\ntrain=dataset.train.path,\ntest=dataset.test.path,\noutput=config.output_predictions_file,\nmeta_results=meta_results_file,\ntask_type=config.type,\ncores=config.cores\n), _live_output_=True)\n</code></pre>   Here, the <code>exec.R</code> script is also responsible to save the predictions in the expected format.</p>"},{"location":"extending/framework/#add-a-default-framework","title":"Add a default framework","text":"<p>Is called \"default framework\" an AutoML framework whose integration is available on <code>master</code> branch under the <code>frameworks</code> folder, and with a simple definition in <code>resources/frameworks.yaml</code>.  </p> <p>NOTE: There are a few requirements when integrating a new default framework:</p> <ul> <li>The code snippet triggering the training should use only defaults (no AutoML hyper parameters), plus possibly a generic <code>**kwargs</code> in order to support <code>params</code> section in custom framework definitions.  In other words, one of the requirements for being included in the benchmark is that the framework is submitted without any tweaks to default settings.  This is to prevent submissions (systems) from overfitting or tuning to the benchmark.</li> <li>There must be a way to limit the runtime of the algorithm (a maximum runtime parameter).</li> <li>Exceptions:</li> <li>the problem type (\"classification\", \"regression\", \"binary\", \"multiclass\"): this is available through <code>config.type</code> or <code>dataset.type</code>. </li> <li>information about data, for example the column types: available through the <code>dataset</code> object.</li> <li>time, cpu and memory constraints: those must be provided by the benchmark application through the <code>config</code> object.  </li> <li>the objective function: provided by <code>config.metric</code> (usually requires a translation for a given framework).</li> <li>seed: provided by <code>config.seed</code></li> <li>paths to folders (output, temporary...): if possible, use <code>config.output_dir</code> or a subfolder (see existing integrations).</li> <li>The default framework definition in <code>resources/frameworks.yaml</code> shouldn't have any <code>params</code> section: this <code>params</code> section is intended for custom definitions, not default ones. <pre><code>good_framework:\nversion: \"0.0.1\"\nproject: \"http://go.to/good_framework\"\nbad_framework:\nversion: \"0.0.1\"\nproject: \"http://go.to/bad_framework\"\nparams: enable_this: true\nuse: ['this', 'that']\n</code></pre></li> </ul> <p>Using the instructions above:</p> <ol> <li>verify that there is an issue created under https://github.com/openml/automlbenchmark/issues for the framework you want to add, or create one.</li> <li>create a private branch for your integration changes.</li> <li>create the framework module (e.g. <code>MyFramework</code>) under <code>frameworks</code> folder.</li> <li>define the module (if possible without any <code>params</code>) in <code>resources/frameworks.yaml</code>.</li> <li>try to setup the framework:      &gt; python runbenchmark.py myframework -s only</li> <li>fixes the framework setup until it works: the setup being usually a simple <code>setup.sh</code> script, you should be able to test it directly without using the application.</li> <li>try to run simple test against one fold using defaults (<code>test</code> benchmark and <code>test</code> constraints) with the <code>-Xtest_mode</code> that will trigger additional validations:     &gt; python runbenchmark.py myframework -f 0 -Xtest_mode</li> <li>fix the module integration code until the test produce all results with no error (if the integration generated an error, it is visible in the results).</li> <li>if this works, validate it against the <code>validation</code> dataset using one fold:     &gt; python runbenchmark.py myframework validation 1h4c -f 0 -Xtest_mode</li> <li>if this works, try to run it in docker to validate the docker image setup:      &gt; python runbenchmark.py myframework -m docker</li> <li>if this works, try to run it in aws:      &gt; python runbenchmark.py myframework -m aws</li> <li>add a brief description of the framework to the documentation in docs/automl_overview following the same formatting as the other entries.</li> <li>create a pull request, and ask a review from authors of <code>automlbenchmark</code>: they'll also be happy to help you during this integration.</li> </ol>"},{"location":"extending/framework/#add-a-custom-framework","title":"Add a custom framework","text":"<p>You may want to integrate a framework without wanting to make this publicly available.</p> <p>In this case, as we've seen above, there's always the possibility to integrate your framework in a custom <code>user_dir</code>.</p> <p>Using the instructions above:</p> <ol> <li>define what is (or will be) your custom <code>user_dir</code> for this framework.</li> <li>ensure it contains a <code>config.yaml</code>, otherwise create one (for example copy this one or <code>examples/custom/config.yaml</code>).</li> <li>create the framework module somewhere under this <code>user_dir</code>, e.g. <code>{user_dir}/extensions/MyFramework</code>.</li> <li>define the module in <code>{user_dir}/frameworks.yaml</code> (create the file if needed).</li> <li>follow the same steps as for a \"default\" framework to implement the integration: setup, test, ... except that you always need to specify the <code>user_dir</code>, e.g. for testing:     &gt; python runbenchmark.py myframework -f 0 -u {user_dir}</li> <li>there may be some issues when trying to build the docker image when the framework is in a custom folder, as all the files should be under the docker build context: solving this probably requires a multi-stage build, needs more investigation. For now, if you really need a docker image, you can either build it manually, or simply copy the <code>extensions</code> folder temporarily under <code>automlbenchmark</code>.</li> <li>even without docker image, you can run the framework on AWS, as soon as the custom <code>config.yaml</code>, <code>frameworks.yaml</code> and <code>extensions</code> folder are made available as AWS resources: cf. again the custom configuration. The application will copy those files to the EC2 instances into a local <code>user_dir</code> and will be able to setup the framework there.</li> </ol>"},{"location":"extending/framework/#using-a-different-hyperparameter-configuration","title":"Using a Different Hyperparameter Configuration","text":"<p>When you want to use an existing framework integration with a different hyperparameter configuration, it is often enough to write only a custom framework definition without further changes. </p> <p>Framework definitions accept a <code>params</code> dictionary for pass-through parameters,  i.e., parameters that are directly accessible from the <code>exec.py</code> file in the framework  integration executing the AutoML training. Most integration scripts use this to overwrite any (default) hyperparameter value. Use the <code>extends</code> field to indicate which framework definition to copy default values from, and then add any fields to overwrite. In the example below the <code>n_estimators</code> and <code>verbose</code> params are passed  directly to the <code>RandomForestClassifier</code>, which will now train only 200 trees (default is 2000):</p> <pre><code>RandomForest_custom:\nextends: RandomForest\nparams:\nn_estimators: 200\nverbose: true\n</code></pre> <p>This new definition can be used as normal:  <pre><code>python runbenchmark.py randomforest_custom ...\n</code></pre></p> <p>Note</p> <p>By convention, param names starting with <code>_</code> are filtered out (they are not passed  to the framework) but are used for custom logic in the <code>exec.py</code>. For example, the <code>_save_artifact</code> field is often used to allow additional artifacts, such as logs or models, to be saved.</p>"},{"location":"using/","title":"Using the Benchmark Tool","text":""},{"location":"using/#installation","title":"Installation","text":""},{"location":"using/aws/","title":"AWS","text":"<p>The AutoML benchmark supports running experiments on AWS EC2.</p> <p>AMLB does not limit expenses!</p> <p>The AWS integration lets your easily conduct massively parallel evaluations. The AutoML Benchmark does not in any way restrict the total costs you can make on AWS. However, there are some tips for reducing costs.</p> Example Costs <p>For example, benchmarking one framework on the classification and regression suites on a one hour budget takes 1 hour * 10 folds * 100 datasets = 1,000 hours, plus overhead. Even when using spot instance pricing on <code>m5.2xlarge</code> instances (default) probably costs at least $100 US (prices depend on overhead and fluctating prices). A full evaluation with multiple frameworks and/or time budgets can cost thousands of dollars. </p>"},{"location":"using/aws/#setup","title":"Setup","text":"<p>To run a benchmark on AWS you additionally need to have a configured AWS account. The application is using the boto3 Python package to exchange files through S3 and create EC2 instances.</p> <p>If this is your first time setting up your AWS account on the machine that will run the  <code>automlbenchmark</code> app, you can use the AWS CLI tool and run:  <pre><code>aws configure\n</code></pre> You will need your AWS Access Key ID, AWS Secret Access Key, and pick a default EC2 region.</p> <p>Selecting a Region</p> <p>To use a region, an AMI must be configured in the automl benchmark configuration file under <code>aws.ec2.regions</code>. The default configuration has AMIs for <code>us-east-1</code>,  <code>us-east-2</code>, <code>us-west-1</code>, <code>eu-west-1</code>, and <code>eu-central-1</code>. If you default EC2 region is different from these, you will need to add the AMI to your custom configuration.</p> <p>On first use, it is recommended to use the following configuration file, or to extend your custom configuration file with these options. Follow the instructions in the file and make any necessary adjustments before running the benchmark.</p> Starting AWS Configuration<pre><code># put this file in your ~/.config/automlbenchmark directory\n# to override default configs\n---\nproject_repository: https://github.com/openml/automlbenchmark\naws:\niam:\ntemporary: false  # set to true if you want IAM entities (credentials used by ec2 instances) being recreated for each benchmark run.\ncredentials_propagation_waiting_time_secs: 360  # increase this waiting time if you encounter credentials issues on ec2 instances when using temporary IAM.\ns3:\nbucket: automl-benchmark-REPLACEME  # ALWAYS SET this bucket name as it needs to be unique in entire S3 domain.\n# (40 chars max as the app reserves some chars for temporary buckets)\n# if you prefer using temporary s3 buckets (see below), you can comment out this property.\ntemporary: false  # set to true if you want a new s3-bucket being temporarily created/deleted for each benchmark run.\nec2:\nterminate_instances: always  # see resources/config.yaml for explanations: you may want to switch this value to `success` if you want to investigate on benchmark failures.\n</code></pre> <p>To run a test to see if the benchmark framework is working on AWS, do the following:</p> <pre><code>python3 runbenchmark.py constantpredictor test -m aws\n</code></pre> <p>This will create and start an EC2 instance for each benchmark job and run the 6 jobs  (3 OpenML tasks * 2 folds) from the <code>test</code> benchmark sequentially. Each job will run is constrained to a one-minute limit in this case, excluding setup  time for the EC2 instances (though <code>constantpredictor</code> will likely only take seconds).</p> <p>For longer benchmarks, you'll probably want to run multiple jobs in parallel and  distribute the work to several EC2 instances, for example: <pre><code>python3 runbenchmark.py autosklearn validation 1h4c -m aws -p 4\n</code></pre> will keep 4 EC2 instances running, monitor them in a dedicated thread, and finally collect all outputs from s3.</p> EC2 Instances always stopped eventually (by default) <p>Each EC2 instance is provided with a time limit at startup to ensure that in any case,  the instance is stopped even if there is an issue when running the benchmark task.  In this case the instance is stopped, not terminated, and we can therefore inspect  the machine manually (ideally after resetting its UserData field to avoid  re-triggering the benchmark on the next startup).</p> <p>The console output is still showing the instances starting, outputs the progress and  then the results for each dataset/fold combination (log excerpt from different command):</p> Example output benchmarking H2O on AWS<pre><code>Running `H2OAutoML_nightly` on `validation` benchmarks in `aws` mode!\nLoading frameworks definitions from ['/Users/me/repos/automlbenchmark/resources/frameworks.yaml'].\nLoading benchmark definitions from /Users/me/repos/automlbenchmark/resources/benchmarks/validationt.yaml.\nUploading `/Users/me/repos/automlbenchmark/resources/benchmarks/validation.yaml` to `ec2/input/validation.yaml` on s3 bucket automl-benchmark.\n...\nStarting new EC2 instance with params: H2OAutoML_nightly /s3bucket/input/validation.yaml -t micro-mass -f 0\nStarted EC2 instance i-0cd081efc97c3bf6f \n[2019-01-22T11:51:32] checking job aws_validation_micro-mass_0_H2OAutoML_nightly on instance i-0cd081efc97c3bf6f: pending \nStarting new EC2 instance with params: H2OAutoML_nightly /s3bucket/input/validation.yaml -t micro-mass -f 1\nStarted EC2 instance i-0251c1655e286897c \n...\n[2019-01-22T12:00:32] checking job aws_validation_micro-mass_1_H2OAutoML_nightly on instance i-0251c1655e286897c: running\n[2019-01-22T12:00:33] checking job aws_validation_micro-mass_0_H2OAutoML_nightly on instance i-0cd081efc97c3bf6f: running\n[2019-01-22T12:00:48] checking job aws_validation_micro-mass_1_H2OAutoML_nightly on instance i-0251c1655e286897c: running\n[2019-01-22T12:00:48] checking job aws_validation_micro-mass_0_H2OAutoML_nightly on instance i-0cd081efc97c3bf6f: running\n...\n[  731.511738] cloud-init[1521]: Predictions saved to /s3bucket/output/predictions/h2oautoml_nightly_micro-mass_0.csv\n[  731.512132] cloud-init[1521]: H2O session _sid_96e7 closed.\n[  731.512506] cloud-init[1521]: Loading predictions from /s3bucket/output/predictions/h2oautoml_nightly_micro-mass_0.csv\n[  731.512890] cloud-init[1521]: Metric scores: {'framework': 'H2OAutoML_nightly', 'version': 'nightly', 'task': 'micro-mass', 'fold': 0, 'mode': 'local', 'utc': '2019-01-22T12:00:02', 'logloss': 0.6498889633819804, 'acc': 0.8793103448275862, 'result': 0.6498889633819804}\n[  731.513275] cloud-init[1521]: Job local_micro-mass_0_H2OAutoML_nightly executed in 608.534 seconds\n[  731.513662] cloud-init[1521]: All jobs executed in 608.534 seconds\n[  731.514089] cloud-init[1521]: Scores saved to /s3bucket/output/scores/H2OAutoML_nightly_task_micro-mass.csv\n[  731.514542] cloud-init[1521]: Loaded scores from /s3bucket/output/scores/results.csv\n[  731.515006] cloud-init[1521]: Scores saved to /s3bucket/output/scores/results.csv\n[  731.515357] cloud-init[1521]: Summing up scores for current run:\n[  731.515782] cloud-init[1521]:          task          framework    ...         acc   logloss\n[  731.516228] cloud-init[1521]: 0  micro-mass  H2OAutoML_nightly    ...     0.87931  0.649889\n[  731.516671] cloud-init[1521]: [1 rows x 9 columns]\n...\nEC2 instance i-0cd081efc97c3bf6f is stopped\nJob aws_validation_micro-mass_0_H2OAutoML_nightly executed in 819.305 seconds\n[2019-01-22T12:01:34] checking job aws_validation_micro-mass_1_H2OAutoML_nightly on instance i-0251c1655e286897c: running\n[2019-01-22T12:01:49] checking job aws_validation_micro-mass_1_H2OAutoML_nightly on instance i-0251c1655e286897c: running\nEC2 instance i-0251c1655e286897c is stopping\nJob aws_validation_micro-mass_1_H2OAutoML_nightly executed in 818.463 seconds\n...\nTerminating EC2 instances i-0251c1655e286897c\nTerminated EC2 instances i-0251c1655e286897c with response {'TerminatingInstances': [{'CurrentState': {'Code': 32, 'Name': 'shutting-down'}, 'InstanceId': 'i-0251c1655e286897c', 'PreviousState': {'Code': 64, 'Name': 'stopping'}}], 'ResponseMetadata': {'RequestId': 'd09eeb0c-7a58-4cde-8f8b-2308a371a801', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'text/xml;charset=UTF-8', 'transfer-encoding': 'chunked', 'vary': 'Accept-Encoding', 'date': 'Tue, 22 Jan 2019 12:01:53 GMT', 'server': 'AmazonEC2'}, 'RetryAttempts': 0}}\nInstance i-0251c1655e286897c state: shutting-down\nAll jobs executed in 2376.891 seconds\nDeleting uploaded resources `['ec2/input/validation.yaml', 'ec2/input/config.yaml', 'ec2/input/frameworks.yaml']` from s3 bucket automl-benchmark.\n</code></pre>"},{"location":"using/aws/#configurable-aws-options","title":"Configurable AWS Options","text":"<p>When using AWS mode, the application will use <code>on-demand</code> EC2 instances from the <code>m5</code>  series by default. However, it is also possible to use <code>Spot</code> instances, specify a  <code>max_hourly_price</code>, or customize your experience when using this mode in general. All configuration points are grouped and documented under the <code>aws</code> yaml namespace in  the main config file. When setting  your own configuration, it is strongly recommended to first create your  own <code>config.yaml</code> file as described in Custom configuration. Here is an example of a config file using Spot instances on a non-default region: <pre><code>aws:\nregion: 'us-east-1'\nresource_files:\n- '{user}/config.yaml'\n- '{user}/frameworks.yaml'\nec2:\nsubnet_id: subnet-123456789   # subnet for account on us-east-1 region\nspot:\nenabled: true\nmax_hourly_price: 0.40  # comment out to use default\n</code></pre></p>"},{"location":"using/aws/#reducing-costs","title":"Reducing Costs","text":"<p>The most important thing you can do to reduce costs is to critically evaluate which experimental results can be re-used from previous publications. That said, when conducting new experiments on AWS we have the following recommendations to reduce costs:</p> <ul> <li>Use spot instances with a fixed maximum price: set <code>aws.ec2.spot.enabled: true</code> and <code>aws.ec2.spot.max_hourly_price</code>.     Check which region has the lowest spot instance prices    and configure <code>aws.region</code> accordingly. </li> <li>Skip the framework installation process by providing a docker image and setting <code>aws.docker_enabled: true</code>.</li> <li>Set up AWS Budgets    to get alerts early if forecasted usage exceeds the budget. It should also be    technically possibly to automatically shut down all running instances in a region    if a budget is exceeded, but this naturally leads to a loss of experimental results, so    it is best avoided.</li> </ul>"},{"location":"using/configuration/","title":"Configuration","text":"<p>The AutoML benchmark has a host of settings that can be configured from a <code>yaml</code> file. It is possible to write your own configuration file that overrides the default behavior in a flexible manner.</p>"},{"location":"using/configuration/#configuration-options","title":"Configuration Options","text":"<p>The default configuration options can be found in the  <code>resources/config.yaml</code> file.</p> resources/config.yaml<pre><code>---\nproject_repository: https://github.com/openml/automlbenchmark#stable      # this is also the url used to clone the repository on ec2 instances\n# when running those without docker.\n# to clone a specific branch/tag, add a url fragment, e.g.:\n# https://github.com/openml/automlbenchmark#stable\nuser_dir:    # where to override settings with a custom config.yaml file and, for example, add custom frameworks, benchmark definitions or framework modules: set by caller (runbenchmark.py).\ninput_dir:   # where the datasets are loaded by default: : set by caller (runbenchmark.py).\noutput_dir:  # where logs and results are saved by default: set by caller (runbenchmark.py).\nroot_dir:    # app root dir: set by caller (runbenchmark.py)\nscript:     # calling script: set by caller (runbenchmark.py)\nrun_mode:   # target run mode (local, docker, aws): set by caller (runbenchmark.py)\nsid:        # session id: set by caller (runbenchmark.py)\njob_history:    # file containing the list of jobs already executed: set by caller (runbenchmark.py)\ntest_mode: false        # if set to true, some additional checks are executed at runtime.\nseed: auto              # default global seed (used if not set in task definition), can be one of:\n#  `auto`: a global seed will be generated and passed to all jobs.\n#  `none`: no seed will be provided (seed left to framework's responsibility).\n#   any int32 to pass a fixed seed to the jobs.\ntoken_separator: '.'    # set to '_' for backwards compatibility.\n# This separator is used to generate directory structure and files,\n# however the '_' separator makes the parsing of those names more difficult as it's also used in framework names, task names...\narchive: ['logs']       # list of output folders that should be archived by default.\nsetup:                   # configuration namespace for the framework setup phase.\nlive_output: true      # set to true to stream the output of setup commands, if false they are only printed when setup is complete.\nactivity_timeout: 600  # when using live output, subprocess will be considered as hanging if nothing was printed during this activity time.\nframeworks:              # configuration namespace for the frameworks definitions.\ndefinition_file:       # list of yaml files describing the frameworks base definitions.\n- '{root}/resources/frameworks.yaml'\nroot_module: frameworks     # the default python module under which the frameworks modules are defined.\nallow_duplicates: false     # if true, the last definition is used.\ntags: ['stable', 'latest', '2020Q2', '2021Q3', '2023Q2']  # the list of supported tags when looking up frameworks:\n# for example frmwk:latest will look for framework frmwk in a frameworks_latest.yaml file if present.\nbenchmarks:                     # configuration namespace for the benchmarks definitions.\ndefinition_dir:               # list of directories containing the benchmarks yaml definitions.\n- '{root}/resources/benchmarks'\nconstraints_file:             # list of yaml files describing the benchmarks runtime constraints.\n- '{root}/resources/constraints.yaml'\non_unfulfilled_constraint: 'auto'  # one of ('auto', 'warn', 'fail'), used when checking the os resources if one memory/core/volume constraint can not be fulfilled:\n#   if 'auto' benchmark, will fail on important unfulfilled constraints (cores, memory) on non local modes, and warn otherwise.\n#   if 'warn' only a warning message will be emitted;\n#   if 'fail' the benchmark will be immediately interrupted;\nos_mem_size_mb: 2048          # the default amount of memory left to the OS when task assigned memory is computed automatically.\nos_vol_size_mb: 2048          # the default amount of volume left to the OS when task volume memory is verified.\noverhead_time_seconds: 3600   # amount of additional time allowed for the job to complete before sending an interruption signal\nmetrics:                      # default metrics by dataset type (as listed by amlb.data.DatasetType),\n# only the first metric is optimized by the frameworks,\n# the others are computed only for information purpose.\nbinary: ['auc', 'logloss', 'acc', 'balacc']     # available metrics: auc (AUC), acc (Accuracy), balacc (Balanced Accuracy), pr_auc (Precision Recall AUC), logloss (Log Loss), f1, f2, f05 (F-beta scores with beta=1, 2, or 0.5), max_pce, mean_pce (Max/Mean Per-Class Error).\nmulticlass: ['logloss', 'acc', 'balacc']        # available metrics: same as for binary, except auc, replaced by auc_ovo (AUC One-vs-One), auc_ovr (AUC One-vs-Rest). AUC metrics and F-beta metrics are computed with weighted average.\nregression: ['rmse', 'r2', 'mae']               # available metrics: mae (Mean Absolute Error), mse (Mean Squared Error), msle (Mean Squared Logarithmic Error), rmse (Root Mean Square Error), rmsle (Root Mean Square Logarithmic Error), r2 (R^2).\ntimeseries: ['mase', 'mape', 'smape', 'rmse', 'mse', 'nrmse', 'wape', 'ncrps']\ndefaults:            # the default constraints, usually overridden by a constraint.\nfolds: 10          # the amount of fold-runs executed for each dataset.\nmax_runtime_seconds: 3600   # default time allocated to the framework to train a model.\ncores: -1                   # default amount of cores used for each automl task. If &lt;= 0, will try to use all cores.\nmax_mem_size_mb: -1         # default amount of memory assigned to each automl task. If &lt;= 0, then the amount of memory is computed from os available memory.\nmin_vol_size_mb: -1         # default minimum amount of free space required on the volume. If &lt;= 0, skips verification.\njob_scheduler:           # configuration namespace\nexit_on_job_failure:   # if true, the entire run will be aborted on the first job failure (mainly used for testing) : set by caller (runbenchmark.py)\nparallel_jobs: 1       # the number of jobs being run in parallel in a benchmark session, set by caller (runbenchmark.py)\nmax_parallel_jobs: 10  # safety limit: increase this if you want to be able to run many jobs in parallel, especially in aws mode. Defaults to 10 to allow running the usual 10 folds in parallel with no problem.\ndelay_between_jobs: 5  # delay in seconds between each parallel job start\nmonitoring:               # configuration namespace describing the basic monitoring features: currently, the app only logs each statistic at a given interval.\ninterval_seconds: 120   # set to &lt;= 0 to disable monitoring\nstatistics: ['cpu', 'sys_memory', 'volume'] # the monitoring values currently available are:\n# cpu : monitors global cpu usage; higher verbosity provides cpu usage per core.\n# sys_memory : monitors the global memory usage; higher verbosity increases the precision: from percent (v=0) to bytes (v&gt;=2).\n# volume : monitors the volume usage; higher verbosity increases the precision, from percent (v=0) to bytes (v&gt;=2).\n# proc_memory : monitors the memory usage on the main process; higher verbosity adds some details.\n# sub_proc_memory : monitors the memory usage on the framework subprocess running in its virtual environment; available only if the main process is run as root.\nverbosity: 0            # from 0 to 3, higher verbosity provides more details for each statistic.\nresults:                 # configuration namespace for the results.csv file.\nerror_max_length: 200  # the max length of the error message as rendered in the results file.\nglobal_save: true      # set by runbenchmark.py, if true adds the results to the main `results.csv` in the {output.dir}\nglobal_lock_timeout: 5 # the timeout used to wait for the lock on the global results file.\nincremental_save: true # if true save results after each job., otherwise save results only when all jobs are completed.\ninference_time_measurements:  # configuration namespace for performing additional inference time measurements on various batch sizes\nenabled: false\nbatch_sizes: [1, 10, 100, 1000, 10000]  # the batch sizes for which inference speed should be measured\nrepeats: 10                            # the number of times to repeat the inference measurement for each batch size\nadditional_job_time: 300  # the time in seconds that will be added to the maximum job time if inference time is measured\nlimit_by_dataset_size: true  # Don't measure inference time on `batch size` if it exceeds the number of rows in the dataset.\n# E.g., on micro-mass (571 rows) with `batch_sizes` [1, 10, 100, 1000, 10000], only measure [1, 10, 100].\nopenml:                # configuration namespace for openML.\napikey: c1994bdb7ecb3c6f3c8f3b35f4b47f1f\ninfer_dtypes: False\nversions:              # configuration namespace for versions enforcement (libraries versions are usually enforced in requirements.txt for the app and for each framework).\npip:\npython: 3.9          # the Python minor version that will be used by the application in containers and cloud instances, also used as a based version for virtual environments created for each framework.\ncontainer: &amp;container          # parent configuration namespace for container modes.\nforce_branch: true           # set to true if image can only be built from a clean branch, with same tag as defined in `project_repository`.\nignore_labels: ['stable']    # branches listed here won't appear in the container name.\nminimize_instances: true     # set to true to avoid running multiple container instances on the same machine.\nrun_extra_options: ''        # additional options passed to the container exec.\nimage:                       # set this value through -Xcontainer.image=my-image to run benchmark with a specific image\nimage_defaults:\nauthor: automlbenchmark\nimage:                     # set by container impl based on framework name, lowercase\ntag:                       # set by container impl based on framework version\ndocker:                        # configuration namespace for docker: it inherits from `container` namespace.\n&lt;&lt;: *container\nrun_extra_options: '--shm-size=2048M'\nrun_as: 'default'    # Sets the user inside the docker container (`docker run -u`), one of:\n#  * 'user': set as `-u $(id -u):$(id -g)`, only on unix systems.\n#  * 'root': set as `-u 0:0`\n#  * 'default': does not set `-u`\n#  * any string that starts with `-u`, which will be directly forwarded.\n# Try this setting if you have problems with permissions in docker.\nbuild_extra_options: ''\nsingularity:                   # configuration namespace for docker: it inherits from `container` namespace.\n&lt;&lt;: *container\nlibrary: 'automlbenchmark/default'\naws:                    # configuration namespace for AWS mode.\nregion: ''            # the AWS region to use. By default, the app will use the region set in ~/.aws/config when configuring AWS CLI.\niam:                                               # sub-namespace for AWS IAM service.\n# Mainly used to allow secure communication between S3 storage and EC2 instances.\nrole_name: AutomlBenchmarkRole                   # must be unique per AWS account, max 40 chars.\n# if temporary is set to true, the generated role name will be `&lt;role_name&gt;-&lt;now&gt;`.\n# cf. commplete restrictions: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-limits.html\ns3_policy_name: AutomlBenchmarkS3Policy\ninstance_profile_name: AutomlBenchmarkProfile    # must be unique per AWS account.\n# if temporary is set to true, the generated instance profile name will be `&lt;instance_profile_name&gt;-&lt;now&gt;`.\ntemporary: false                                 # if true, the IAM entities will be automatically recreated during setup and deleted at the end of the benchmark run.\ncredentials_propagation_waiting_time_secs: 360   # time to wait before being able to start ec2 instances when using new or temporary credentials.\nmax_role_session_duration_secs: 7200             # the max duration (in seconds) during which the ec2 instance will have access to s3.\n# This should be a number between 900 (15mn) to 43200 (12h).\ns3:                               # sub-namespace for AWS S3 service.\nbucket: automl-benchmark        # must be unique im whole Amazon s3, max 40 chars, and include only numbers, lowercase characters and hyphens.\n# if temporary is set to true, the generated bucket name will be `&lt;bucket&gt;-&lt;now&gt;`.\n# cf. complete restrictions: https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html\ntemporary: false                # if true, the S3 bucket is created during setup and deleted at the end of the benchmark run.\n# Note that for safety reasons, the bucket is then created with a generated name: &lt;s3.bucket&gt;-&lt;now&gt;.\n# if false, the real &lt;s3.bucket&gt; name is used (after creation if it doesn't exists), but never deleted.\nroot_key: ec2/                  #\ndelete_resources: false         #\nec2:\nregions:                             #\nus-east-1:\nami: ami-053b0d53c279acc90\ndescription: Canonical, Ubuntu, 22.04 LTS, amd64 jammy image build on 2023-05-16\nus-east-2:\nami: ami-024e6efaf93d85776\ndescription: Canonical, Ubuntu, 22.04 LTS, amd64 jammy image build on 2023-05-16\nus-west-1:\nami: ami-0f8e81a3da6e2510a\ndescription: Canonical, Ubuntu, 22.04 LTS, amd64 jammy image build on 2023-05-16\neu-west-1:\nami: ami-01dd271720c1ba44f\ndescription: Canonical, Ubuntu, 22.04 LTS, amd64 jammy image build on 2023-05-16\neu-central-1:\nami: ami-04e601abe3e1a910f\ndescription: Canonical, Ubuntu, 22.04 LTS, amd64 jammy image build on 2023-05-16\ninstance_type:                       #\nseries: m5                         #\nmap:                               # map between num cores required and ec2 instance type sizes\ndefault: large\n'1': small\n'2': large\n'4': xlarge\n'8': 2xlarge\n'16': 4xlarge\ninstance_tags: {}                    # specify custom tags for the EC2 instances here\nvolume_type: standard                # one of gp2, io1, st1, sc1, or standard (default).\nvolume_tags: {}                      # specify custom tags for the volume tags here (if empty, will apply the same tags as the corresponding instance)\nroot_device_name: '/dev/sda1'        #\navailability_zone:                   # the availability zone where the instances will be created (if not set, aws will pick a default one).\nsubnet_id: ''                        #\nkey_name:                            # the name of the key pair passed to EC2 instances (if not set, user can't ssh the running instances)\nsecurity_groups: []                  # the optional additional security groups to set on the instances\nterminate_instances: always          # if `always`, the EC2 instances are always terminated.\n# if `success`, EC2 instances are terminated at the end of the main job iff it ended successfully (=the main results could be downloaded),\n#               otherwise the instance is just stopped and open to manual investigation after restart in case of issue\n#               (don't forget to delete the instance UserData before restarting it).\n# if `never`, the instances are only stopped.\nterminate_waiter:                    # the config used to wait for instance complete stop or termination (to completely disable the waiter, set it to None, or set max_attempts to 0)\ndelay: 0                           # delay between each request during waiting period: 0 defaults to `aws.query_frequency_seconds` instead of aws defaults (15).\nmax_attempts: 40                   # max requests during waiting period: using aws defaults (40)\nspot:                                #\nenabled: false                     # if enabled, aws mode will try to obtain a spot instance instead of on-demand.\nblock_enabled: false               # if enabled, and if spot is enabled, aws mode will try to use block instances (possible only if total instance runtime &lt;= 6h, i.e. for benchmark runtime up to 4h).\nmax_hourly_price: ''               # the max hourly price (in dollar) per instance to bid (defaults to on-demand price).\nfallback_to_on_demand: false       # if we couldn't obtain any spot instance after all attempts in the job scheduling logic, then starts an on-demand instance.\nmonitoring:                          # EC2 instances monitoring\ncpu:                               #\nperiod_minutes: 5                #\ndelta_minutes: 30                #\nthreshold: 5                     #\nabort_inactive_instances: true   # stop/terminate instance if its cpu activity was lower than `threshold` %, for all periods or `period_minutes` in the last `delta_minutes`.\nquery_interval_seconds: 300      # set to &lt;= 0 to disable\njob_scheduler:                             # AWS mode sub-namespace specifying\nmax_attempts: 10                         #\nretry_policy: 'exponential:300:2:10800'  # use \"constant:interval\", \"linear:start:increment:max\" or \"exponential:start:factor:max\"\n# e.g. \"linear:300:600\" will first wait 5min and then add 10min to waiting time between each retry,\n#      \"exponential:300:2:10800\" with first wait 5min and then double waiting time between each retry, until the maximum of 3h then used for all retries.\nretry_on_errors:                         # Boto3 errors that will trigger a job reschedule.\n- 'SpotMaxPriceTooLow'\n- 'MaxSpotInstanceCountExceeded'\n- 'InsufficientFreeAddressesInSubnet'\n- 'InsufficientInstanceCapacity'\n- 'VolumeLimitExceeded'\nretry_on_states:                         # EC2 instance states that will trigger a job reschedule.\n- 'Server.SpotInstanceShutdown'\n- 'Server.SpotInstanceTermination'\n- 'Server.InsufficientInstanceCapacity'\n- 'Client.VolumeLimitExceeded'\nmax_timeout_seconds: 21600    #\nos_mem_size_mb: 0             # overrides the default amount of memory left to the os in AWS mode, and set to 0 for fairness as we can't always prevent frameworks from using all available memory.\noverhead_time_seconds: 1800   # amount of additional time allowed for the job to complete on aws before the instance is stopped.\nquery_interval_seconds: 30    # check instance state every N seconds\nresource_files: []            # additional resource files or directories that are made available to benchmark runs on ec2, from remote input or user directory.\n# Those files are actually uploaded to s3 bucket (precisely to s3://{s3.bucket}/{s3.root_key}/user),\n#  this folder being itself synchronized on each ec2 instance and used as user directory.\n# The possibility of adding resource_files is especially necessary to run custom frameworks.\nresource_ignore:              # files ignored when listing `resource_files`, especially if those contain directories.\n- '*/lib/*'\n- '*/venv/*'\n- '*/__pycache__/*'\n- '*/.marker_*'\n- '*.swp'\nminimize_instances: false    # if true,\nuse_docker: false            # if true, EC2 instances will run benchmark tasks in a docker instance.\n# if false, it will run in local mode after cloning project_repository.\n# Note that using docker in AWS mode requires the docker image being\n# previously published in a public repository or using an AMI with the pre-downloaded image,\n# whereas the local mode is self-configured and framework agnostic (works with generic AMI).\n</code></pre>"},{"location":"using/configuration/#custom-configurations","title":"Custom Configurations","text":"<p>To override default configuration, create your custom <code>config.yaml</code> file under the <code>user_dir</code> (specified by the <code>--userdir</code> parameter of <code>runbenchmark.py</code>). The application will automatically load this custom file and apply it on top of the defaults.</p> <p>When specifying filepaths, configurations support the following placeholders:</p> Placeholder Replaced By Value Of Default Function <code>{input}</code> <code>input_dir</code> <code>~/.openml/cache</code> Folder from which datasets are loaded (and/or downloaded) <code>{output}</code> <code>output_dir</code> <code>./results</code> Folder where all outputs (results, logs, predictions, ...) are stored. <code>{user}</code> <code>user_dir</code> <code>~/.config/automlbenchmark</code> Folder containing custom configuration files. <code>{root}</code> <code>root_dir</code> Detected at runtime The root folder of the <code>automlbenchmark</code> application. <p>For example, including the following snippet in your custom configuration when <code>user_dir</code> is <code>~/.config/automlbenchmark</code> (which it is by default) changes your  input directory to <code>~/.config/automlbenchmark/data</code> :</p> examples/custom/config.yaml<pre><code>input_dir: '{user}/data'   # change the default input directory (where data files are loaded and/or downloaded).\n</code></pre> <p>Multiple Configuration Files</p> <p>It is possible to have multiple configuration files:  just create a folder for each <code>config.yaml</code> file and use that folder as your  <code>user_dir</code> using <code>--userdir /path/to/config/folder</code> when invoking <code>runbenchmark.py</code>.</p> <p>Below is an example of a configuration file which 1. changes the directory the  datasets are loaded from, 2. specifies additional paths to look up framework, benchmark, and constraint definitions, 3. also makes those available in an S3 bucket  when running in AWS mode, and 4. changes the default EC2 instance type for AWS mode.</p> examples/custom/config.yaml<pre><code>---\nproject_repository: https://github.com/openml/automlbenchmark\ninput_dir: '{user}/data'   # change the default input directory (where data files are loaded and/or downloaded).\nframeworks:\ndefinition_file:  # this allows to add custom framework definitions (in {user}/frameworks.yaml) on top of the default ones.\n- '{root}/resources/frameworks.yaml'\n- '{user}/frameworks.yaml'\nbenchmarks:\ndefinition_dir:  # this allows to add custom benchmark definitions (under {user}/benchmarks) to the default ones.\n- '{user}/benchmarks'\n- '{root}/resources/benchmarks'\nconstraints_file: # this allows to add custom constraint definitions (in {user}/constraints.yaml) on top of the default ones.\n- '{root}/resources/constraints.yaml'\n- '{user}/constraints.yaml'\naws:\nresource_files:  # this allows to automatically upload custom config + frameworks to the running instance (benchmark files are always uploaded).\n- '{user}/config.yaml'\n- '{user}/frameworks.yaml'\n- '{user}/constraints.yaml'\n- '{user}/benchmarks'\n- '{user}/extensions'\nec2:\ninstance_type:\nseries: t3\nuse_docker: false  # you can decide to always use the prebuilt docker images on AWS.\n</code></pre>"},{"location":"using/parameters/","title":"Parameters of <code>runbenchmark.py</code>","text":"<p>The parameters of the <code>runbenchmark.py</code> script can be shown with:</p> python runbenchmark.py --help<pre><code>usage: runbenchmark.py [-h] [-m {local,aws,docker,singularity}] [-t [task_id ...]] [-f [fold_num ...]] [-i input_dir] [-o output_dir] [-u user_dir] [-p parallel_jobs] [-s {auto,skip,force,only}] [-k [true|false]]\n                       [-e] [--logging LOGGING] [--openml-run-tag OPENML_RUN_TAG]\n                       framework [benchmark] [constraint]\n\npositional arguments:\n  framework             The framework to evaluate as defined by default in resources/frameworks.yaml.\n                        To use a labelled framework (i.e. a framework defined in resources/frameworks-{label}.yaml),\n                        use the syntax {framework}:{label}.\n  benchmark             The benchmark type to run as defined by default in resources/benchmarks/{benchmark}.yaml,\n                        a path to a benchmark description file, or an openml suite or task.\n                        OpenML references should be formatted as 'openml/s/X' and 'openml/t/Y',\n                        for studies and tasks respectively. Use 'test.openml/s/X' for the \n                        OpenML test server.\n                        (default: 'test')\n  constraint            The constraint definition to use as defined by default in resources/constraints.yaml.\n                        (default: 'test')\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -m {local,aws,docker,singularity}, --mode {local,aws,docker,singularity}\n                        The mode that specifies how/where the benchmark tasks will be running.\n                        (default: 'local')\n  -t [task_id ...], --task [task_id ...]\n                        The specific task name (as defined in the benchmark file) to run.\n                        When an OpenML reference is used as benchmark, the dataset name should be used instead.\n                        If not provided, then all tasks from the benchmark will be run.\n  -f [fold_num ...], --fold [fold_num ...]\n                        If task is provided, the specific fold(s) to run.\n                        If fold is not provided, then all folds from the task definition will be run.\n  -i input_dir, --indir input_dir\n                        Folder from where the datasets are loaded by default.\n                        (default: '/Users/pietergijsbers/.openml')\n  -o output_dir, --outdir output_dir\n                        Folder where all the outputs should be written.(default: '/Users/pietergijsbers/repositories/forks/automlbenchmark-fork/results')\n  -u user_dir, --userdir user_dir\n                        Folder where all the customizations are stored.(default: '~/.config/automlbenchmark')\n  -p parallel_jobs, --parallel parallel_jobs\n                        The number of jobs (i.e. tasks or folds) that can run in parallel.\n                        A hard limit is defined by property `job_scheduler.max_parallel_jobs`\n                         in `resources/config.yaml`.\n                        Override this limit in your custom `config.yaml` file if needed.\n                        Supported only in aws mode or container mode (docker, singularity).\n                        (default: 1)\n  -s {auto,skip,force,only}, --setup {auto,skip,force,only}\n                        Framework/platform setup mode. Available values are:\n                        \u2022 auto: setup is executed only if strictly necessary.\n                        \u2022 skip: setup is skipped.\n                        \u2022 force: setup is always executed before the benchmark.\n                        \u2022 only: only setup is executed (no benchmark).\n                        (default: 'auto')\n  -k [true|false], --keep-scores [true|false]\n                        Set to true (default) to save/add scores in output directory.\n  -e, --exit-on-error   If set, terminates on the first task that does not complete with a model.\n  --logging LOGGING     Set the log levels for the 3 available loggers:\n                        \u2022 console\n                        \u2022 app: for the log file including only logs from amlb (.log extension).\n                        \u2022 root: for the log file including logs from libraries (.full.log extension).\n                        Accepted values for each logger are: notset, debug, info, warning, error, fatal, critical.\n                        Examples:\n                          --logging=info (applies the same level to all loggers)\n                          --logging=root:debug (keeps defaults for non-specified loggers)\n                          --logging=console:warning,app:info\n                        (default: 'console:info,app:debug,root:info')\n  --openml-run-tag OPENML_RUN_TAG\n                        Tag that will be saved in metadata and OpenML runs created during upload, must match '([a-zA-Z0-9_\\-\\.])+'.\n</code></pre>"},{"location":"using/result_analysis/","title":"Results","text":"<p>The AutoML benchmark produces many result files, such as logs, performance records, and meta-data of the experiments. Some of these files can also be automatically parsed and visualized by notebooks we provide.</p>"},{"location":"using/result_analysis/#output-file-structure","title":"Output File Structure","text":"<p>Except the logs, all the files generated by the application are in easy to process  <code>csv</code> or <code>json</code> format, and they are all generated in a subfolder of the <code>output_dir</code>  unique for each benchmark run.</p> <p>For example: <pre><code>results/randomforest.test.test.local.20201204T192714\n|-- predictions\n|   |-- cholesterol\n|   |   |-- 0\n|   |   |   |-- metadata.json\n|   |   |   `-- predictions.csv\n|   |   `-- 1\n|   |       |-- metadata.json\n|   |       `-- predictions.csv\n|   |-- iris\n|   |   |-- 0\n|   |   |   |-- metadata.json\n|   |   |   `-- predictions.csv\n|   |   `-- 1\n|   |       |-- metadata.json\n|   |       `-- predictions.csv\n|   `-- kc2\n|       |-- 0\n|       |   |-- metadata.json\n|       |   `-- predictions.csv\n|       `-- 1\n|           |-- metadata.json\n|           `-- predictions.csv\n`-- scores\n    |-- RandomForest.benchmark_test.csv\n    `-- results.csv\n</code></pre></p>"},{"location":"using/result_analysis/#resultscsv","title":"<code>results.csv</code>","text":"<p>Here is a sample <code>results.csv</code> file from a test run against the <code>RandomForest</code> framework:</p> Produced CSVReadable Table <pre><code>id,task,framework,constraint,fold,result,metric,mode,version,params,tag,utc,duration,models,seed,info,acc,auc,balacc,logloss,mae,r2,rmse\nopenml.org/t/3913,kc2,RandomForest,test,0,0.865801,auc,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:27:46,3.2,2000,2633845682,,0.792453,0.865801,0.634199,0.350891,,,\nopenml.org/t/3913,kc2,RandomForest,test,1,0.86039,auc,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:27:52,3.0,2000,2633845683,,0.90566,0.86039,0.772727,0.406952,,,\nopenml.org/t/59,iris,RandomForest,test,0,0.126485,logloss,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:27:56,2.9,2000,2633845682,,0.933333,,0.933333,0.126485,,,\nopenml.org/t/59,iris,RandomForest,test,1,0.0271781,logloss,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:28:01,3.0,2000,2633845683,,1.0,,1.0,0.0271781,,,\nopenml.org/t/2295,cholesterol,RandomForest,test,0,44.3352,rmse,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:28:05,3.0,2000,2633845682,,,,,,35.6783,-0.014619,44.3352\nopenml.org/t/2295,cholesterol,RandomForest,test,1,55.3163,rmse,local,0.23.2,{'n_estimators': 2000},,2020-12-04T19:28:10,3.1,2000,2633845683,,,,,,43.1808,-0.0610752,55.3163\n</code></pre> <pre><code>                  id         task     framework constraint fold     result   metric   mode version                  params                  utc  duration models        seed       acc       auc    balacc   logloss      mae        r2     rmse\n0  openml.org/t/3913          kc2  RandomForest       test    0   0.865801      auc  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:27:46       3.2   2000  2633845682  0.792453  0.865801  0.634199  0.350891      NaN       NaN      NaN\n1  openml.org/t/3913          kc2  RandomForest       test    1   0.860390      auc  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:27:52       3.0   2000  2633845683  0.905660  0.860390  0.772727  0.406952      NaN       NaN      NaN\n2    openml.org/t/59         iris  RandomForest       test    0   0.126485  logloss  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:27:56       2.9   2000  2633845682  0.933333       NaN  0.933333  0.126485      NaN       NaN      NaN\n3    openml.org/t/59         iris  RandomForest       test    1   0.027178  logloss  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:28:01       3.0   2000  2633845683  1.000000       NaN  1.000000  0.027178      NaN       NaN      NaN\n4  openml.org/t/2295  cholesterol  RandomForest       test    0  44.335200     rmse  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:28:05       3.0   2000  2633845682       NaN       NaN       NaN       NaN  35.6783 -0.014619  44.3352\n5  openml.org/t/2295  cholesterol  RandomForest       test    1  55.316300     rmse  local  0.23.2  {'n_estimators': 2000}  2020-12-04T19:28:10       3.1   2000  2633845683       NaN       NaN       NaN       NaN  43.1808 -0.061075  55.3163\n</code></pre> <p>Here is a short description of each column:</p> <ul> <li><code>id</code>: a identifier for the dataset used in this result. For convenience, we use the link to the OpenML task by default.</li> <li><code>task</code>: the task name as defined in the benchmark definition.</li> <li><code>framework</code>: the framework name as defined in the framework definition.</li> <li><code>fold</code>: the dataset fold being used for this job. Usually, we're using 10 folds, so the fold varies from 0 to 9.</li> <li><code>result</code>: the result score, this is the score for the metric that the framework was trying to optimize. For example, for binary classification, the default metrics defined in <code>resources/config.yaml</code> are <code>binary: ['auc', 'acc']</code>; this means that the frameworks should try to optimize <code>auc</code> and the final <code>auc</code> score will become the <code>result</code> value, the other metrics (here <code>acc</code>) are then computed for information.</li> <li><code>mode</code>: one of <code>local</code>, <code>docker</code>, <code>aws</code>, <code>aws+docker</code>: tells where/how the job was executed.</li> <li><code>version</code>: the version of the framework being benchmarked.</li> <li><code>params</code>: if any, a JSON representation of the params defined in the framework definition. This allows to see clearly if some tuning was done for example.</li> <li><code>tag</code>: the branch tag of the <code>automlbenchmark</code> app that was running the job.</li> <li><code>utc</code>: the UTC timestamp at the job completion.</li> <li><code>duration</code>: the training duration: the framework integration is supposed to provide this information to ensure that it takes only into account the time taken by the framework itself. When benchmarking large data, the application can use a significant amount of time to prepare the data: this additional time doesn't appear in this <code>duration</code> column.</li> <li><code>models</code>: for some frameworks, it is possible to know how many models in total were trained by the AutoML framework. </li> <li><code>seed</code>: the seed or random state passed to the framework. With some frameworks, it is enough to obtain reproducible results. Note that the seed can be specified at the command line using <code>-Xseed=</code> arg (for example <code>python randomforest -Xseed=1452956522</code>): when there are multiple folds, the seed is then incremented by the fold number.</li> <li><code>info</code>: additional info in text format, this usually contains error messages if the job failed.</li> <li><code>acc</code>, <code>auc</code>, <code>logloss</code> metrics: all the metrics that were computed based on the generated predictions. For each job/row, one of them matches the <code>result</code> column, the others are purely informative. Those additional metric columns are simply added in alphabetical order.</li> </ul>"},{"location":"using/result_analysis/#predictions-directory","title":"Predictions Directory","text":"<p>For each evaluation, the framework integration must generate a predictions file that will be used by the application to compute the scores. This predictions file is saved  under the <code>predictions</code> subfolder as shown above and follows the naming convention: <code>{framework}_{task}_{fold}.csv</code>.</p> <p>The <code>csv</code> file contains a header row and contains the following columns, in order:   - For classification tasks only, there is first one column per class, sorted alphabetically.     Each column contains the probability of the sample belonging to that class, as predicted by the AutoML framework.     If a framework does not provide probabilities, it will be 1 for the predicted class and 0 otherwise.   - <code>predictions</code>: contains the predictions of the test predictor data by the model trained by the framework,   - <code>truth</code>: the true values of the test target data (<code>test.y</code>).</p> <p>Here are examples of the first few samples for <code>KC2</code> (binary classification),  <code>iris</code> (multiclass classification), and <code>cholesterol</code> (regression):</p> KC2 (csv)KC2 (table) <pre><code>no,yes,predictions,truth\n0.965857617846013,0.034142382153998944,no,no\n0.965857617846013,0.034142382153998944,no,no\n0.5845,0.4155,no,no\n0.6795,0.3205,no,no\n0.965857617846013,0.034142382153998944,no,no\n</code></pre> no yes predictions truth 0.965857617846013 0.034142382153998944 no no 0.965857617846013 0.034142382153998944 no no 0.5845 0.4155 no no 0.6795 0.3205 no no 0.965857617846013 0.034142382153998944 no no iris (csv)iris (table) <pre><code>Iris-setosa,Iris-versicolor,Iris-virginica,predictions,truth\n1.0,0.0,0.0,Iris-setosa,Iris-setosa\n0.9715,0.028,0.0005,Iris-setosa,Iris-setosa\n1.0,0.0,0.0,Iris-setosa,Iris-setosa\n1.0,0.0,0.0,Iris-setosa,Iris-setosa\n1.0,0.0,0.0,Iris-setosa,Iris-setosa\n0.0,1.0,0.0,Iris-versicolor,Iris-versicolor\n</code></pre> Iris-setosa Iris-versicolor Iris-virginica predictions truth 1.0 0.0 0.0 Iris-setosa Iris-setosa 0.9715 0.028 0.0005 Iris-setosa Iris-setosa 1.0 0.0 0.0 Iris-setosa Iris-setosa 1.0 0.0 0.0 Iris-setosa Iris-setosa 1.0 0.0 0.0 Iris-setosa Iris-setosa 0.0 1.0 0.0 Iris-versicolor Iris-versicolor cholesterol (csv)cholesterol (table) <pre><code>predictions,truth\n241.204,207.0\n248.9575,249.0\n302.278,268.0\n225.9215,234.0\n226.6995,201.0\n</code></pre> predictions truth 241.204 207.0 248.9575 249.0 302.278 268.0 225.9215 234.0 226.6995 201.0"},{"location":"using/result_analysis/#extract-more-information","title":"Extract more information","text":"<p>For some frameworks, it is also possible to extract more detailed information,  in the form of <code>artifacts</code> that are saved after the training. Examples of those artifacts are logs generated by the framework, models or descriptions  of the models trained by the framework, predictions for each of the model trained by the AutoML framework. By default, those artifacts are not saved, and not all frameworks  provide the same artifacts. This is why the artifacts to be stored have to be specified in the framework definition (before running the experiments!). By convention,  this can be achieved by specifying the <code>params._save_artifacts</code> parameter. For example: </p> autosklearnH2OTPOT <p>Save model descriptions under the <code>models</code> subfolder: <pre><code>autosklearn_debug:\nextends: autosklearn\nparams:\n_save_artifacts: ['models'] </code></pre></p> <p>Save the leaderboard and models under the <code>models</code> subfolder,  and the H2O logs under <code>logs</code> subfolder: <pre><code>H2OAutoML_debug:\nextends: H2OAutoML\nparams:\n_save_artifacts: ['leaderboard', 'logs', 'models'] </code></pre></p> <p>Save the description of models for the Pareto frontin the <code>models</code> subfolder: <pre><code>TPOT_debug:\nextends: TPOT\nparams:\n_save_artifacts: ['models']\n</code></pre></p> <p>The framework integrations themselves determine where the artifacts are saved, this is typically not configurable from the framework definition.</p>"},{"location":"using/upload_to_openml/","title":"Upload to openml","text":""},{"location":"using/upload_to_openml/#uploading-results-to-openml","title":"Uploading results to OpenML","text":"<p>The <code>upload_results.py</code> script can be used to upload results to OpenML with the following usage: <pre><code>&gt;python upload_results.py --help\nusage: Script to upload results from the benchmark to OpenML. [-h] [-i INPUT_DIRECTORY] [-a APIKEY] [-m MODE] [-x] [-v] [-t TASK]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i INPUT_DIRECTORY, --input-directory INPUT_DIRECTORY\n                        Directory that stores results from the runbenchmark.py invocation. By default use the most recent folder in the results folder as\n                        specified in the configuration.\n  -a APIKEY, --api-key APIKEY\n                        OpenML API key to use for uploading results.\n  -m MODE, --mode MODE  Run mode (default=check).\n                        \u2022 check: only report whether results can be uploaded.\n                        \u2022 upload: upload all complete results.\n  -x, --fail-fast       Stop as soon as a task fails to upload due to an error during uploading.\n  -v, --verbose         Output progress to console.\n  -t TASK, --task TASK  Only upload results for this specific task.\n</code></pre></p> <p>Note that the default behavior does not upload data but only verifies data is complete. We strongly encourage you to only upload your data after verifying all expected results are complete. The OpenML Python package is used for uploading results, so to ensure your API credentials are configured, please refer to their configuration documentation. Results obtained on tasks on the test server (e.g. through the <code>--test-server</code> option of <code>runbenchmark.py</code>) are uploaded to the test server and don't require additional authentication.</p>"}]}